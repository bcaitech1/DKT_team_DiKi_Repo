{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93636e67",
   "metadata": {},
   "source": [
    "# 연속형변수 추가한 baseline코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa65a72-1bf1-44f7-9da7-6a21914459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "from dkt.utils import setSeeds, increment_path, delete_model\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.trainer import compute_loss, update_params, get_lr, save_checkpoint\n",
    "from dkt.metric import get_metric\n",
    "from dkt.criterion import get_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa52652-164e-47e6-bcf3-e1f11c3e85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = {\n",
    "    'seed' : 42,\n",
    "    'device' : 'cuda',\n",
    "    'data_dir' : '/opt/ml/input/data/train_dataset',\n",
    "    'asset_dir' : '/opt/ml/asset/',\n",
    "    'file_name' : 'train_data.csv',\n",
    "    'model_dir' : '/opt/ml/models/',\n",
    "    'model_name' : 'model.pt',\n",
    "    'output_dir' : '/opt/ml/output/',\n",
    "    'test_file_name' : 'test_data.csv',\n",
    "    'max_seq_len' : 128,\n",
    "    'num_workers' : 1,\n",
    "    'hidden_dim' : 512,\n",
    "    'n_layers' : 2,\n",
    "    'n_heads' : 2,\n",
    "    'drop_out' : 0.2,\n",
    "    'n_epochs' : 200,\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.0001,\n",
    "    'clip_grad' : 10,\n",
    "    'patience' : 15,\n",
    "    'log_steps' : 50,\n",
    "    'model' : 'lstm',\n",
    "    'optimizer' : 'adam',\n",
    "    'scheduler' : 'plateau'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**namespace)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.device = device\n",
    "setSeeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb675f0e-7713-4a65-8882-5bf887908de9",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "\n",
    " preprocess\n",
    "\n",
    "- __feature_engineering함수에서 연속형 변수 처리해줬고 그에 맞게 다른 함수도 처리했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9098a1c4-d70a-4a19-82de-fe7bcdaa9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if str(x) in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        def convert_time(s):\n",
    "            timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "            return int(timestamp)\n",
    "\n",
    "        df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        # TODO\n",
    "        def percentile(s):\n",
    "            return np.sum(s) / len(s)\n",
    "        \n",
    "        # 큰 카테고리\n",
    "        df['big_features'] = df['testId'].apply(lambda x : x[2]).astype(int)\n",
    "\n",
    "        # 큰 카테고리별 정답률\n",
    "        stu_groupby = df.groupby('big_features').agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # tag별 정답률\n",
    "        stu_tag_groupby = df.groupby(['big_features', 'KnowledgeTag']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 시험지별 정답률\n",
    "        stu_test_groupby = df.groupby(['big_features', 'testId']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "                                                                    \n",
    "        # 문항별 정답률\n",
    "        stu_assessment_groupby = df.groupby(['big_features', 'assessmentItemID']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'assessmentItemID' : 'assessment_count', 'answerCode' : 'answer_rate'})\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "\n",
    "        # 정답 - 큰 카테고리별 정답률 \n",
    "        '''ex)\n",
    "        맞은 문제의 큰 카테고리별 정답률이 0.7 이면 1 - 0.7 = 0.3이 됨)\n",
    "        틀린 문제의 큰 카테고리별 정답률이 0.7 이면 0 - 0.7 = -0.7이 됨)\n",
    "        '''\n",
    "        temp = pd.merge(df, stu_groupby.reset_index()[['big_features', 'answer_rate']], on = ['big_features'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['answer_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 태그별 정답률\n",
    "        temp = pd.merge(df, stu_tag_groupby.reset_index()[['answer_rate', 'KnowledgeTag']], on = ['KnowledgeTag'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['tag_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 시험별 정답률\n",
    "        temp = pd.merge(df, stu_test_groupby.reset_index()[['answer_rate', 'testId']], on = ['testId'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['test_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 문항별 정답률\n",
    "        temp = pd.merge(df, stu_assessment_groupby.reset_index()[['answer_rate', 'assessmentItemID']], on = ['assessmentItemID'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['assess_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        self.args.n_big_features = 9\n",
    "\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag', 'big_features', 'answer_delta', 'tag_delta', 'test_delta', 'assess_delta']\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['big_features'].values,\n",
    "                    r['answer_delta'].values,\n",
    "                    r['tag_delta'].values,\n",
    "                    r['test_delta'].values,\n",
    "                    r['assess_delta'].values,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ab77a3-fff8-4265-92ff-acbb5819fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(args.file_name)\n",
    "train_data = preprocess.get_train_data()\n",
    "\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd256ef1",
   "metadata": {},
   "source": [
    "## DKTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b281f95-809a-48fe-b797-a2c4eb4607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct, big_features, answer_delta, tag_delta, test_delta, assess_delta = row\n",
    "\n",
    "        # category변수와 continuout변수를 나눠줌        \n",
    "        cate_cols = [test, question, tag, correct, big_features]\n",
    "        cont_cols = [answer_delta, tag_delta, test_delta, assess_delta]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols, cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    # cate변수에서 했던 처리를 cont에서도 똑같이 해줌\n",
    "    cate_col_n = len(batch[0][0])\n",
    "    cont_col_n = len(batch[0][1])\n",
    "\n",
    "    cate_col_list = [[] for _ in range(cate_col_n)]\n",
    "    cont_col_list = [[] for _ in range(cont_col_n)]\n",
    "\n",
    "    max_seq_len = len(batch[0][0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row[0]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cate_col_list[i].append(pre_padded)\n",
    "        for i, col in enumerate(row[1]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cont_col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(cate_col_list):\n",
    "        cate_col_list[i] =torch.stack(cate_col_list[i])\n",
    "    \n",
    "    for i, _ in enumerate(cont_col_list):\n",
    "        cont_col_list[i] =torch.stack(cont_col_list[i])\n",
    "\n",
    "    return tuple(cate_col_list), tuple(cont_col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ba5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33151d8e",
   "metadata": {},
   "source": [
    "### process_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9607527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, args):\n",
    "\n",
    "    (test, question, tag, correct, big_features, mask), cont_features = batch    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "    big_features = big_features.type(torch.FloatTensor)\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "    big_features = (big_features * mask).to(torch.int64)\n",
    "\n",
    "    # interaction과 동일하게 rolling을 해서 이전 정보를 사용할 수 있도록 함\n",
    "    for cont_feature in cont_features:\n",
    "        cont_feature = cont_feature.type(torch.FloatTensor)\n",
    "        cont_feature = cont_feature.roll(shifts=1, dims=1)\n",
    "        cont_feature[:, 0] = 0\n",
    "        cont_feature = (cont_feature * mask).unsqueeze(-1)\n",
    "        temp.append(cont_feature)\n",
    "    \n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "    interaction = interaction.to(args.device)\n",
    "    big_features = big_features.to(args.device)\n",
    "\n",
    "    # 연속형 변수들을 concat해줌\n",
    "    cont_features = torch.cat(temp, dim=-1).to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, big_features), cont_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d4bad",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "- lstm, bert 모델을 사용했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4ea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # 큰 카테고리 embedding 추가\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "        \n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*5, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(4, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "\n",
    "        # cate변수와 cont변수를 concat해서 lstm input으로 넣어줌        \n",
    "        X = torch.cat([cate_embed, cont_embed], 2)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd4170c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        # 큰 카테고리 embedding 추가\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*5, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(4, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # Bert config\n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=self.args.n_layers,\n",
    "            num_attention_heads=self.args.n_heads,\n",
    "            max_position_embeddings=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "\n",
    "        # cate변수와 cont변수를 concat해서 bert의 input에 넣어줌        \n",
    "        X = torch.cat([cate_embed, cont_embed], 2)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers[0]\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36854e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    # loss = loss[:,-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921abc09",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ccd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "\n",
    "        # cont변수가 추가되었으므로 처리가 필요함 어쨋든 ground truth 인 정답\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "\n",
    "        # 마찬가지로 정답\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074e295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.model = 'custombert'\n",
    "\n",
    "if args.model == 'customlstm':\n",
    "    model = CustomLSTM(args)\n",
    "elif args.model == 'custombert':\n",
    "    model = CustomBert(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d723e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcha-no\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rural-microwave-41</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/cha-no/DKT\" target=\"_blank\">https://wandb.ai/cha-no/DKT</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/cha-no/DKT/runs/p1zfh83d\" target=\"_blank\">https://wandb.ai/cha-no/DKT/runs/p1zfh83d</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/wandb/run-20210528_081138-p1zfh83d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 저는 추가한 feature를 wandb에 기록해주었습니다\n",
    "args.add_features = [\"bigfeature\",\"answer_delta\",\"tag_delta\",\"test_delta\",\"assess_delta\"]\n",
    "name = 'fe4_maxseq128_hiddendim512_custombert'\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project='dkt', config=vars(args))\n",
    "wandb.run.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8dd7c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.7785154581069946\n",
      "Training steps: 50 Loss: 0.46498364210128784\n",
      "TRAIN AUC : 0.6951391503564113 ACC : 0.6279863481228669\n",
      "VALID AUC : 0.7348494090319868 ACC : 0.6467661691542289\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.40450945496559143\n",
      "Training steps: 50 Loss: 0.42555931210517883\n",
      "TRAIN AUC : 0.7167595794356313 ACC : 0.6380119453924915\n",
      "VALID AUC : 0.7384189212912133 ACC : 0.6477611940298508\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.4592132270336151\n",
      "Training steps: 50 Loss: 0.47890979051589966\n",
      "TRAIN AUC : 0.720167160935248 ACC : 0.6431313993174061\n",
      "VALID AUC : 0.7380142641637433 ACC : 0.6606965174129353\n",
      "\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.4624432921409607\n",
      "Training steps: 50 Loss: 0.44153642654418945\n",
      "TRAIN AUC : 0.7171360926678377 ACC : 0.6446245733788396\n",
      "VALID AUC : 0.7389346607674004 ACC : 0.6597014925373135\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.432332307100296\n",
      "Training steps: 50 Loss: 0.4490538239479065\n",
      "TRAIN AUC : 0.7222573107838677 ACC : 0.6446245733788396\n",
      "VALID AUC : 0.7380628626913072 ACC : 0.6522388059701493\n",
      "\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.44292640686035156\n",
      "Training steps: 50 Loss: 0.43670058250427246\n",
      "TRAIN AUC : 0.7211920427164746 ACC : 0.6456911262798635\n",
      "VALID AUC : 0.7386133154014685 ACC : 0.6537313432835821\n",
      "\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.42416974902153015\n",
      "Training steps: 50 Loss: 0.4390999674797058\n",
      "TRAIN AUC : 0.7218221781791893 ACC : 0.6431313993174061\n",
      "VALID AUC : 0.7415406328321734 ACC : 0.6517412935323383\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.43848711252212524\n",
      "Training steps: 50 Loss: 0.442901611328125\n",
      "TRAIN AUC : 0.7272556378981991 ACC : 0.6456911262798635\n",
      "VALID AUC : 0.7469097783212879 ACC : 0.6656716417910448\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.43874770402908325\n",
      "Training steps: 50 Loss: 0.4484937787055969\n",
      "TRAIN AUC : 0.7373285064276187 ACC : 0.6563566552901023\n",
      "VALID AUC : 0.7544455255137311 ACC : 0.6616915422885572\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.4155276119709015\n",
      "Training steps: 50 Loss: 0.4077306091785431\n",
      "TRAIN AUC : 0.7421364801209218 ACC : 0.6572098976109215\n",
      "VALID AUC : 0.7622510441244875 ACC : 0.6651741293532338\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.42718732357025146\n",
      "Training steps: 50 Loss: 0.40404170751571655\n",
      "TRAIN AUC : 0.7450772034456886 ACC : 0.6646757679180887\n",
      "VALID AUC : 0.7682296548215195 ACC : 0.681592039800995\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.43932944536209106\n",
      "Training steps: 50 Loss: 0.41231828927993774\n",
      "TRAIN AUC : 0.7508507102242944 ACC : 0.6712883959044369\n",
      "VALID AUC : 0.7698165455174801 ACC : 0.6716417910447762\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.4065692126750946\n",
      "Training steps: 50 Loss: 0.41303908824920654\n",
      "TRAIN AUC : 0.7574428825780126 ACC : 0.6793941979522184\n",
      "VALID AUC : 0.7719370282099575 ACC : 0.6865671641791045\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.45042479038238525\n",
      "Training steps: 50 Loss: 0.41438716650009155\n",
      "TRAIN AUC : 0.755993352216733 ACC : 0.6776877133105802\n",
      "VALID AUC : 0.7681155970527472 ACC : 0.6885572139303483\n",
      "\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.3910662829875946\n",
      "Training steps: 50 Loss: 0.4089307188987732\n",
      "TRAIN AUC : 0.755456752486765 ACC : 0.6796075085324232\n",
      "VALID AUC : 0.7724289643257054 ACC : 0.6835820895522388\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.41634857654571533\n",
      "Training steps: 50 Loss: 0.44239315390586853\n",
      "TRAIN AUC : 0.7636143264652336 ACC : 0.6896331058020477\n",
      "VALID AUC : 0.7718894214890788 ACC : 0.6825870646766169\n",
      "\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.37667638063430786\n",
      "Training steps: 50 Loss: 0.42822086811065674\n",
      "TRAIN AUC : 0.7602592562541765 ACC : 0.691339590443686\n",
      "VALID AUC : 0.7776904987894999 ACC : 0.6955223880597015\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.462072491645813\n",
      "Training steps: 50 Loss: 0.4193664789199829\n",
      "TRAIN AUC : 0.7581167774478602 ACC : 0.6870733788395904\n",
      "VALID AUC : 0.7751802360698272 ACC : 0.6870646766169154\n",
      "\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.441201388835907\n",
      "Training steps: 50 Loss: 0.3833268880844116\n",
      "TRAIN AUC : 0.7617100629314975 ACC : 0.6906996587030717\n",
      "VALID AUC : 0.7799240474440645 ACC : 0.6950248756218905\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.3963373899459839\n",
      "Training steps: 50 Loss: 0.4441298246383667\n",
      "TRAIN AUC : 0.7609786426743561 ACC : 0.6926194539249146\n",
      "VALID AUC : 0.7758120169281566 ACC : 0.6990049751243781\n",
      "\n",
      "Start Training: Epoch 21\n",
      "Training steps: 0 Loss: 0.41616034507751465\n",
      "Training steps: 50 Loss: 0.40156835317611694\n",
      "TRAIN AUC : 0.7634492258687381 ACC : 0.6915529010238908\n",
      "VALID AUC : 0.7692908879744431 ACC : 0.6945273631840796\n",
      "\n",
      "Start Training: Epoch 22\n",
      "Training steps: 0 Loss: 0.4253268539905548\n",
      "Training steps: 50 Loss: 0.37914028763771057\n",
      "TRAIN AUC : 0.7677283488937531 ACC : 0.691339590443686\n",
      "VALID AUC : 0.776239485609381 ACC : 0.6975124378109453\n",
      "\n",
      "Start Training: Epoch 23\n",
      "Training steps: 0 Loss: 0.39550819993019104\n",
      "Training steps: 50 Loss: 0.39135682582855225\n",
      "TRAIN AUC : 0.7641466414199198 ACC : 0.6949658703071673\n",
      "VALID AUC : 0.7805786398561483 ACC : 0.6980099502487562\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 24\n",
      "Training steps: 0 Loss: 0.42329537868499756\n",
      "Training steps: 50 Loss: 0.42044177651405334\n",
      "TRAIN AUC : 0.7612552385936091 ACC : 0.6949658703071673\n",
      "VALID AUC : 0.7730855403511591 ACC : 0.6855721393034826\n",
      "\n",
      "Start Training: Epoch 25\n",
      "Training steps: 0 Loss: 0.4352613091468811\n",
      "Training steps: 50 Loss: 0.3852277994155884\n",
      "TRAIN AUC : 0.7690739506630918 ACC : 0.6928327645051194\n",
      "VALID AUC : 0.778573206739128 ACC : 0.7054726368159204\n",
      "\n",
      "Start Training: Epoch 26\n",
      "Training steps: 0 Loss: 0.4048669934272766\n",
      "Training steps: 50 Loss: 0.3935474157333374\n",
      "TRAIN AUC : 0.7637481573187153 ACC : 0.6968856655290102\n",
      "VALID AUC : 0.7739970106946514 ACC : 0.6965174129353234\n",
      "\n",
      "Start Training: Epoch 27\n",
      "Training steps: 0 Loss: 0.37019985914230347\n",
      "Training steps: 50 Loss: 0.40723732113838196\n",
      "TRAIN AUC : 0.7642598688858764 ACC : 0.6938993174061433\n",
      "VALID AUC : 0.7734624268914497 ACC : 0.6965174129353234\n",
      "\n",
      "Start Training: Epoch 28\n",
      "Training steps: 0 Loss: 0.4154544472694397\n",
      "Training steps: 50 Loss: 0.39234045147895813\n",
      "TRAIN AUC : 0.7670108769476354 ACC : 0.6960324232081911\n",
      "VALID AUC : 0.7754024007672616 ACC : 0.7049751243781095\n",
      "\n",
      "Start Training: Epoch 29\n",
      "Training steps: 0 Loss: 0.3717504143714905\n",
      "Training steps: 50 Loss: 0.44731205701828003\n",
      "TRAIN AUC : 0.7623251560980101 ACC : 0.6943259385665529\n",
      "VALID AUC : 0.777329481156169 ACC : 0.7049751243781095\n",
      "\n",
      "Start Training: Epoch 30\n",
      "Training steps: 0 Loss: 0.40445810556411743\n",
      "Training steps: 50 Loss: 0.40291833877563477\n",
      "TRAIN AUC : 0.7673800969453198 ACC : 0.699018771331058\n",
      "VALID AUC : 0.7803415980584392 ACC : 0.7014925373134329\n",
      "\n",
      "Start Training: Epoch 31\n",
      "Training steps: 0 Loss: 0.3720608353614807\n",
      "Training steps: 50 Loss: 0.41040879487991333\n",
      "TRAIN AUC : 0.763938054912586 ACC : 0.695179180887372\n",
      "VALID AUC : 0.7771936036403273 ACC : 0.6985074626865672\n",
      "\n",
      "Start Training: Epoch 32\n",
      "Training steps: 0 Loss: 0.38108569383621216\n",
      "Training steps: 50 Loss: 0.40753400325775146\n",
      "TRAIN AUC : 0.7646842439696345 ACC : 0.6943259385665529\n",
      "VALID AUC : 0.7742866182466643 ACC : 0.6950248756218905\n",
      "\n",
      "Start Training: Epoch 33\n",
      "Training steps: 0 Loss: 0.3761053681373596\n",
      "Training steps: 50 Loss: 0.4244703948497772\n",
      "TRAIN AUC : 0.7618558364565091 ACC : 0.6919795221843004\n",
      "VALID AUC : 0.7785246082115642 ACC : 0.6925373134328359\n",
      "\n",
      "Start Training: Epoch 34\n",
      "Training steps: 0 Loss: 0.44882798194885254\n",
      "Training steps: 50 Loss: 0.42485302686691284\n",
      "TRAIN AUC : 0.7680295594795183 ACC : 0.6985921501706485\n",
      "VALID AUC : 0.7756582868919852 ACC : 0.7044776119402985\n",
      "\n",
      "Epoch    34: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Start Training: Epoch 35\n",
      "Training steps: 0 Loss: 0.40809568762779236\n",
      "Training steps: 50 Loss: 0.3860970735549927\n",
      "TRAIN AUC : 0.7666504088313777 ACC : 0.6992320819112628\n",
      "VALID AUC : 0.7815069709132854 ACC : 0.6990049751243781\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 36\n",
      "Training steps: 0 Loss: 0.3750879168510437\n",
      "Training steps: 50 Loss: 0.40646839141845703\n",
      "TRAIN AUC : 0.7723766919164523 ACC : 0.7049914675767918\n",
      "VALID AUC : 0.7790681182749308 ACC : 0.7089552238805971\n",
      "\n",
      "Start Training: Epoch 37\n",
      "Training steps: 0 Loss: 0.42512601613998413\n",
      "Training steps: 50 Loss: 0.36181071400642395\n",
      "TRAIN AUC : 0.7692816255161103 ACC : 0.7032849829351536\n",
      "VALID AUC : 0.7794648409489211 ACC : 0.7099502487562189\n",
      "\n",
      "Start Training: Epoch 38\n",
      "Training steps: 0 Loss: 0.4092726707458496\n",
      "Training steps: 50 Loss: 0.42202985286712646\n",
      "TRAIN AUC : 0.7684445445238293 ACC : 0.7045648464163823\n",
      "VALID AUC : 0.7810923957189656 ACC : 0.6960199004975124\n",
      "\n",
      "Start Training: Epoch 39\n",
      "Training steps: 0 Loss: 0.4152364134788513\n",
      "Training steps: 50 Loss: 0.40896254777908325\n",
      "TRAIN AUC : 0.7695604094057198 ACC : 0.699018771331058\n",
      "VALID AUC : 0.7810824776521159 ACC : 0.6980099502487562\n",
      "\n",
      "Start Training: Epoch 40\n",
      "Training steps: 0 Loss: 0.43249017000198364\n",
      "Training steps: 50 Loss: 0.4021153151988983\n",
      "TRAIN AUC : 0.7680247277116474 ACC : 0.7037116040955631\n",
      "VALID AUC : 0.7778055483649571 ACC : 0.7029850746268657\n",
      "\n",
      "Start Training: Epoch 41\n",
      "Training steps: 0 Loss: 0.4200785756111145\n",
      "Training steps: 50 Loss: 0.42715272307395935\n",
      "TRAIN AUC : 0.7693283022170521 ACC : 0.6994453924914675\n",
      "VALID AUC : 0.7789381915991991 ACC : 0.6970149253731344\n",
      "\n",
      "Start Training: Epoch 42\n",
      "Training steps: 0 Loss: 0.3989070653915405\n",
      "Training steps: 50 Loss: 0.41555652022361755\n",
      "TRAIN AUC : 0.7719194061119128 ACC : 0.7054180887372014\n",
      "VALID AUC : 0.780755181446074 ACC : 0.6975124378109453\n",
      "\n",
      "Start Training: Epoch 43\n",
      "Training steps: 0 Loss: 0.43192005157470703\n",
      "Training steps: 50 Loss: 0.4093315303325653\n",
      "TRAIN AUC : 0.7675885011217907 ACC : 0.6996587030716723\n",
      "VALID AUC : 0.7811112400459801 ACC : 0.6955223880597015\n",
      "\n",
      "Start Training: Epoch 44\n",
      "Training steps: 0 Loss: 0.41590601205825806\n",
      "Training steps: 50 Loss: 0.4015473425388336\n",
      "TRAIN AUC : 0.7679581769466327 ACC : 0.6953924914675768\n",
      "VALID AUC : 0.7805493815589416 ACC : 0.6990049751243781\n",
      "\n",
      "Start Training: Epoch 45\n",
      "Training steps: 0 Loss: 0.40358489751815796\n",
      "Training steps: 50 Loss: 0.39777058362960815\n",
      "TRAIN AUC : 0.7720080189113571 ACC : 0.7037116040955631\n",
      "VALID AUC : 0.7798040388351826 ACC : 0.7039800995024875\n",
      "\n",
      "Start Training: Epoch 46\n",
      "Training steps: 0 Loss: 0.3944048285484314\n",
      "Training steps: 50 Loss: 0.43274644017219543\n",
      "TRAIN AUC : 0.7708121107805858 ACC : 0.702005119453925\n",
      "VALID AUC : 0.7771251689790639 ACC : 0.7059701492537314\n",
      "\n",
      "Epoch    46: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Start Training: Epoch 47\n",
      "Training steps: 0 Loss: 0.4191245138645172\n",
      "Training steps: 50 Loss: 0.4500484764575958\n",
      "TRAIN AUC : 0.7735911977952552 ACC : 0.7084044368600683\n",
      "VALID AUC : 0.7834042971016433 ACC : 0.7089552238805971\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 48\n",
      "Training steps: 0 Loss: 0.37611210346221924\n",
      "Training steps: 50 Loss: 0.3762212097644806\n",
      "TRAIN AUC : 0.770445261084121 ACC : 0.6996587030716723\n",
      "VALID AUC : 0.7779136552936194 ACC : 0.700497512437811\n",
      "\n",
      "Start Training: Epoch 49\n",
      "Training steps: 0 Loss: 0.41776442527770996\n",
      "Training steps: 50 Loss: 0.42238837480545044\n",
      "TRAIN AUC : 0.7712198937558061 ACC : 0.7071245733788396\n",
      "VALID AUC : 0.781279847182426 ACC : 0.7089552238805971\n",
      "\n",
      "Start Training: Epoch 50\n",
      "Training steps: 0 Loss: 0.42174333333969116\n",
      "Training steps: 50 Loss: 0.3975437879562378\n",
      "TRAIN AUC : 0.7715629492746423 ACC : 0.70669795221843\n",
      "VALID AUC : 0.779990498491958 ACC : 0.6965174129353234\n",
      "\n",
      "Start Training: Epoch 51\n",
      "Training steps: 0 Loss: 0.40825626254081726\n",
      "Training steps: 50 Loss: 0.4230555295944214\n",
      "TRAIN AUC : 0.7730591563368635 ACC : 0.7054180887372014\n",
      "VALID AUC : 0.7803663932255637 ACC : 0.7044776119402985\n",
      "\n",
      "Start Training: Epoch 52\n",
      "Training steps: 0 Loss: 0.3888879418373108\n",
      "Training steps: 50 Loss: 0.4290441870689392\n",
      "TRAIN AUC : 0.771922870398311 ACC : 0.7015784982935154\n",
      "VALID AUC : 0.7790324132342716 ACC : 0.7014925373134329\n",
      "\n",
      "Start Training: Epoch 53\n",
      "Training steps: 0 Loss: 0.3620433211326599\n",
      "Training steps: 50 Loss: 0.386452317237854\n",
      "TRAIN AUC : 0.7748250307455418 ACC : 0.7109641638225256\n",
      "VALID AUC : 0.7817529389711593 ACC : 0.7049751243781095\n",
      "\n",
      "Start Training: Epoch 54\n",
      "Training steps: 0 Loss: 0.4297873377799988\n",
      "Training steps: 50 Loss: 0.3919847011566162\n",
      "TRAIN AUC : 0.770909201965162 ACC : 0.7084044368600683\n",
      "VALID AUC : 0.7814038230180479 ACC : 0.699502487562189\n",
      "\n",
      "Start Training: Epoch 55\n",
      "Training steps: 0 Loss: 0.3727683424949646\n",
      "Training steps: 50 Loss: 0.38788336515426636\n",
      "TRAIN AUC : 0.7736499083331587 ACC : 0.7047781569965871\n",
      "VALID AUC : 0.7810467726114567 ACC : 0.7064676616915423\n",
      "\n",
      "Start Training: Epoch 56\n",
      "Training steps: 0 Loss: 0.3992800712585449\n",
      "Training steps: 50 Loss: 0.40077537298202515\n",
      "TRAIN AUC : 0.7713115150144907 ACC : 0.7081911262798635\n",
      "VALID AUC : 0.7817430209043094 ACC : 0.7054726368159204\n",
      "\n",
      "Start Training: Epoch 57\n",
      "Training steps: 0 Loss: 0.4181180000305176\n",
      "Training steps: 50 Loss: 0.3516571521759033\n",
      "TRAIN AUC : 0.7721721166881058 ACC : 0.7047781569965871\n",
      "VALID AUC : 0.7837087817539308 ACC : 0.709452736318408\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 58\n",
      "Training steps: 0 Loss: 0.40103358030319214\n",
      "Training steps: 50 Loss: 0.4234651029109955\n",
      "TRAIN AUC : 0.7730091976803868 ACC : 0.7045648464163823\n",
      "VALID AUC : 0.7801263760077995 ACC : 0.7054726368159204\n",
      "\n",
      "Start Training: Epoch 59\n",
      "Training steps: 0 Loss: 0.3973323702812195\n",
      "Training steps: 50 Loss: 0.3941776752471924\n",
      "TRAIN AUC : 0.7725050528440425 ACC : 0.7052047781569966\n",
      "VALID AUC : 0.7817489717444193 ACC : 0.7024875621890547\n",
      "\n",
      "Start Training: Epoch 60\n",
      "Training steps: 0 Loss: 0.41894134879112244\n",
      "Training steps: 50 Loss: 0.3892384171485901\n",
      "TRAIN AUC : 0.7725382370611182 ACC : 0.7094709897610921\n",
      "VALID AUC : 0.7808077472003776 ACC : 0.708457711442786\n",
      "\n",
      "Start Training: Epoch 61\n",
      "Training steps: 0 Loss: 0.3722737431526184\n",
      "Training steps: 50 Loss: 0.41984623670578003\n",
      "TRAIN AUC : 0.7715423458871171 ACC : 0.7039249146757679\n",
      "VALID AUC : 0.7815972253216181 ACC : 0.7059701492537314\n",
      "\n",
      "Start Training: Epoch 62\n",
      "Training steps: 0 Loss: 0.38464438915252686\n",
      "Training steps: 50 Loss: 0.3722236752510071\n",
      "TRAIN AUC : 0.7751437450941601 ACC : 0.7073378839590444\n",
      "VALID AUC : 0.7821566042919443 ACC : 0.7019900497512438\n",
      "\n",
      "Start Training: Epoch 63\n",
      "Training steps: 0 Loss: 0.41341081261634827\n",
      "Training steps: 50 Loss: 0.39758747816085815\n",
      "TRAIN AUC : 0.7724704099800621 ACC : 0.7088310580204779\n",
      "VALID AUC : 0.7827992950238083 ACC : 0.709452736318408\n",
      "\n",
      "Start Training: Epoch 64\n",
      "Training steps: 0 Loss: 0.39852070808410645\n",
      "Training steps: 50 Loss: 0.4143180251121521\n",
      "TRAIN AUC : 0.7724205424890169 ACC : 0.7069112627986348\n",
      "VALID AUC : 0.780672861491221 ACC : 0.7029850746268657\n",
      "\n",
      "Start Training: Epoch 65\n",
      "Training steps: 0 Loss: 0.36262571811676025\n",
      "Training steps: 50 Loss: 0.38547664880752563\n",
      "TRAIN AUC : 0.7700891689085767 ACC : 0.702005119453925\n",
      "VALID AUC : 0.7824025723498181 ACC : 0.7024875621890547\n",
      "\n",
      "Start Training: Epoch 66\n",
      "Training steps: 0 Loss: 0.4429357945919037\n",
      "Training steps: 50 Loss: 0.40627309679985046\n",
      "TRAIN AUC : 0.7713519924660888 ACC : 0.7056313993174061\n",
      "VALID AUC : 0.7803683768389336 ACC : 0.7014925373134329\n",
      "\n",
      "Start Training: Epoch 67\n",
      "Training steps: 0 Loss: 0.3566007614135742\n",
      "Training steps: 50 Loss: 0.391102135181427\n",
      "TRAIN AUC : 0.7722661994134417 ACC : 0.7049914675767918\n",
      "VALID AUC : 0.7827050733887355 ACC : 0.7104477611940299\n",
      "\n",
      "Start Training: Epoch 68\n",
      "Training steps: 0 Loss: 0.38536372780799866\n",
      "Training steps: 50 Loss: 0.39868712425231934\n",
      "TRAIN AUC : 0.7723856261287421 ACC : 0.7092576791808873\n",
      "VALID AUC : 0.7823073589080605 ACC : 0.7064676616915423\n",
      "\n",
      "Epoch    68: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Start Training: Epoch 69\n",
      "Training steps: 0 Loss: 0.40396660566329956\n",
      "Training steps: 50 Loss: 0.36678484082221985\n",
      "TRAIN AUC : 0.7762041813936824 ACC : 0.7073378839590444\n",
      "VALID AUC : 0.7829986481674884 ACC : 0.7134328358208956\n",
      "\n",
      "Start Training: Epoch 70\n",
      "Training steps: 0 Loss: 0.35836541652679443\n",
      "Training steps: 50 Loss: 0.39622312784194946\n",
      "TRAIN AUC : 0.774928685841188 ACC : 0.70669795221843\n",
      "VALID AUC : 0.7822409078601671 ACC : 0.7079601990049751\n",
      "\n",
      "Start Training: Epoch 71\n",
      "Training steps: 0 Loss: 0.38407519459724426\n",
      "Training steps: 50 Loss: 0.3768630027770996\n",
      "TRAIN AUC : 0.7750479302256253 ACC : 0.7120307167235495\n",
      "VALID AUC : 0.7821476780317795 ACC : 0.7074626865671642\n",
      "\n",
      "Start Training: Epoch 72\n",
      "Training steps: 0 Loss: 0.4317575693130493\n",
      "Training steps: 50 Loss: 0.3860844671726227\n",
      "TRAIN AUC : 0.7744710353749225 ACC : 0.70669795221843\n",
      "VALID AUC : 0.7831821324042089 ACC : 0.7029850746268657\n",
      "\n",
      "EarlyStopping counter: 15 out of 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27522<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/wandb/run-20210528_081138-p1zfh83d/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/wandb/run-20210528_081138-p1zfh83d/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>71</td></tr><tr><td>train_loss</td><td>0.39894</td></tr><tr><td>train_auc</td><td>0.77447</td></tr><tr><td>train_acc</td><td>0.7067</td></tr><tr><td>valid_auc</td><td>0.78318</td></tr><tr><td>valid_acc</td><td>0.70299</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>_runtime</td><td>633</td></tr><tr><td>_timestamp</td><td>1622190131</td></tr><tr><td>_step</td><td>71</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▆▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▁▃▃▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███████████████</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▆▇▇▇▇█▇▇▇███▇▇█▇██████</td></tr><tr><td>valid_auc</td><td>▁▂▂▂▃▅▆▆▆▇▇▆▇▆▇▇█▇▇█▇█▇██▇▇▇████████████</td></tr><tr><td>valid_acc</td><td>▁▁▂▂▃▃▅▅▅▆▆▆▇▅▇▇▇▇▆▇█▇▇▇▇█▇▇▇▇██▇█▇▇▇██▇</td></tr><tr><td>lr</td><td>███████████████████▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">rural-microwave-41</strong>: <a href=\"https://wandb.ai/cha-no/DKT/runs/p1zfh83d\" target=\"_blank\">https://wandb.ai/cha-no/DKT/runs/p1zfh83d</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.model_dir = increment_path(os.path.join(args.model_dir, args.model))\n",
    "os.makedirs(args.model_dir, exist_ok=True)    \n",
    "\n",
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "        \n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    \n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "    \n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    lr = get_lr(optimizer)\n",
    "    ### TODO: model save or early stopping\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                \"valid_auc\":auc, \"valid_acc\":acc, \"lr\":lr})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        \n",
    "        # 이함수는 제가 추가한 함수라서 주석처리해주시면 됩니다\n",
    "        # delete_model(args.model_dir)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model_to_save.state_dict(),\n",
    "            },\n",
    "            args.model_dir, f'model_{epoch + 1}.pt',\n",
    "        )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1df4d7",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c869a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(args.test_file_name)\n",
    "test_data = preprocess.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdcfcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_loader = get_loaders(args, None, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28ce964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):    \n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    print(\"Loading Model from:\", model_path)\n",
    "    load_state = torch.load(model_path)\n",
    "\n",
    "    if args.model == 'custombert':\n",
    "        model = CustomBert(args)\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "    \n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d562aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from: /opt/ml/models/custombert/model_57.pt\n",
      "Loading Model from: /opt/ml/models/custombert/model_57.pt ...Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.model_name = 'model_57.pt'\n",
    "model = load_model(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb13c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args, test_data):\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "    \n",
    "    os.makedirs(os.path.join(args.output_dir, args.model), exist_ok=True)\n",
    "    write_path = os.path.join(args.output_dir, args.model, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    print(write_path)\n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48e74a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/output/custombert/output.csv\n",
      "writing prediction : /opt/ml/output/custombert/output.csv\n"
     ]
    }
   ],
   "source": [
    "inference(args, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
