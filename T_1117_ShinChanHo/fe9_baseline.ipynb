{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa65a72-1bf1-44f7-9da7-6a21914459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from transformers import XLMRobertaModel, XLMRobertaConfig \n",
    "\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "from dkt.utils import setSeeds, increment_path, delete_model\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.trainer import compute_loss, update_params, get_lr, save_checkpoint\n",
    "from dkt.metric import get_metric\n",
    "from dkt.criterion import get_criterion\n",
    "from dkt.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa52652-164e-47e6-bcf3-e1f11c3e85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = {\n",
    "    'seed' : 42,\n",
    "    'device' : 'cuda',\n",
    "    'data_dir' : '/opt/ml/input/data/train_dataset',\n",
    "    'asset_dir' : '/opt/ml/asset/',\n",
    "    'file_name' : 'train_data.csv',\n",
    "    'model_dir' : '/opt/ml/models/',\n",
    "    'model_name' : 'model.pt',\n",
    "    'output_dir' : '/opt/ml/output/',\n",
    "    'test_file_name' : 'test_data.csv',\n",
    "    'max_seq_len' : 128,\n",
    "    'window' : True,\n",
    "    'shuffle' : False,\n",
    "    'shuffle_n' : 2,\n",
    "    'num_workers' : 1,\n",
    "    'hidden_dim' : 512,\n",
    "    'n_layers' : 2,\n",
    "    'n_heads' : 2,\n",
    "    'drop_out' : 0.2,\n",
    "    'n_epochs' : 200,\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.0001,\n",
    "    'clip_grad' : 10,\n",
    "    'patience' : 15,\n",
    "    'log_steps' : 50,\n",
    "    'model' : 'lstm',\n",
    "    'optimizer' : 'adam',\n",
    "    'scheduler' : 'plateau'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**namespace)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.stride = args.max_seq_len\n",
    "args.device = device\n",
    "setSeeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb675f0e-7713-4a65-8882-5bf887908de9",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9098a1c4-d70a-4a19-82de-fe7bcdaa9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if str(x) in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        # def convert_time(s):\n",
    "        #     timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "        #     return int(timestamp)\n",
    "\n",
    "        # df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        # 큰 카테고리\n",
    "        df['grade'] = df['testId'].str[2].astype(int)\n",
    "        \n",
    "        # 이전 문제 소모시간 추가\n",
    "        df['tmp_index'] = df.index\n",
    "        tmp_df = df[['userID', 'testId', 'Timestamp', 'tmp_index']].shift(1)\n",
    "        tmp_df['tmp_index'] += 1\n",
    "        tmp_df = tmp_df.rename(columns={'Timestamp':'prior_timestamp'})\n",
    "        df = df.merge(tmp_df, how='left', on=['userID', 'testId', 'tmp_index'])\n",
    "        df['prior_elapsed'] = (df.Timestamp - df.prior_timestamp).dt.seconds\n",
    "\n",
    "        upper_bound = df['prior_elapsed'].quantile(0.98) # outlier 설정\n",
    "        median = df[df['prior_elapsed'] <= upper_bound]['prior_elapsed'].median() \n",
    "        df.loc[df['prior_elapsed'] > upper_bound, 'prior_elapsed'] = median \n",
    "        df['prior_elapsed'] = df['prior_elapsed'].fillna(median) # 빈값 채우기\n",
    "\n",
    "        df['prior_elapsed'] = np.log1p(df['prior_elapsed']) #\n",
    "        df['prior_elapsed'] = QuantileTransformer(output_distribution='normal').fit_transform(df.prior_elapsed.values.reshape(-1,1)).reshape(-1) \n",
    "\n",
    "        # 문제 평균 소모시간 추가\n",
    "        assess_time = df.groupby('assessmentItemID').prior_elapsed.mean()\n",
    "        assess_time.name = 'mean_elapsed'\n",
    "        df = df.merge(assess_time, how='left', on=['assessmentItemID'])\n",
    "\n",
    "        # 테스트 평균 소모시간 추가\n",
    "        test_time = df.groupby('testId').prior_elapsed.mean()\n",
    "        test_time.name = 'test_time'\n",
    "        df = df.merge(test_time, how='left', on=['testId'])\n",
    "\n",
    "        # 대분류별 평균 소모시간 추가\n",
    "        grade_time = df.groupby('grade').prior_elapsed.mean()\n",
    "        grade_time.name = 'grade_time'\n",
    "        df = df.merge(grade_time, how='left', on=['grade'])\n",
    "\n",
    "        # 수치형 로그\n",
    "        # df['mean_elapsed'] = np.log1p(df['mean_elapsed'])\n",
    "        # df['test_time'] = np.log1p(df['test_time'])\n",
    "        # df['grade_time'] = np.log1p(df['grade_time'])\n",
    "\n",
    "        # user&태그별 누적 카운트\n",
    "        # df['tag_cumCount'] = df.groupby(['userID', 'KnowledgeTag']).cumcount()\n",
    "        # df['tag_cumCount'] = np.log1p(df['tag_cumCount'])\n",
    "\n",
    "        # user&태그별 누적 정답횟수\n",
    "        df['tag_cumAnswer'] = df.groupby(['userID', 'KnowledgeTag']).answerCode.cumsum() - df['answerCode']\n",
    "        df['tag_cumAnswer'] = np.log1p(df['tag_cumAnswer'])\n",
    "\n",
    "        #TODO\n",
    "        def percentile(s):\n",
    "            return np.sum(s) / len(s)\n",
    "\n",
    "        df = df.reset_index()\n",
    "        # 큰 카테고리\n",
    "        df['big_features'] = df['testId'].apply(lambda x : x[2]).astype(int)\n",
    "\n",
    "        # 큰 카테고리별 정답률\n",
    "        stu_groupby = df.groupby('big_features').agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # tag별 정답률\n",
    "        stu_tag_groupby = df.groupby(['big_features', 'KnowledgeTag']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 시험지별 정답률\n",
    "        stu_test_groupby = df.groupby(['big_features', 'testId']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 문항별 정답률\n",
    "        stu_assessment_groupby = df.groupby(['big_features', 'assessmentItemID']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'assessmentItemID' : 'assessment_count', 'answerCode' : 'answer_rate'})\n",
    "\n",
    "        df = df.sort_values(by='index', axis=0)\n",
    "\n",
    "        # 정답 - 큰 카테고리별 정답률 \n",
    "        '''ex)\n",
    "        맞은 문제의 큰 카테고리별 정답률이 0.7 이면 1 - 0.7 = 0.3이 됨)\n",
    "        틀린 문제의 큰 카테고리별 정답률이 0.7 이면 0 - 0.7 = -0.7이 됨)\n",
    "        '''\n",
    "        temp = pd.merge(df, stu_groupby.reset_index()[['big_features', 'answer_rate']], on = ['big_features'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['big_mean'] = temp['answer_rate']\n",
    "        df['answer_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 태그별 정답률\n",
    "        temp = pd.merge(df, stu_tag_groupby.reset_index()[['answer_rate', 'big_features', 'KnowledgeTag']], on = ['big_features', 'KnowledgeTag'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['tag_mean'] = temp['answer_rate']\n",
    "        df['tag_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 시험별 정답률\n",
    "        temp = pd.merge(df, stu_test_groupby.reset_index()[['answer_rate', 'big_features', 'testId']], on = ['big_features', 'testId'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['test_mean'] = temp['answer_rate']\n",
    "        df['test_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 문항별 정답률\n",
    "        temp = pd.merge(df, stu_assessment_groupby.reset_index()[['answer_rate', 'big_features', 'assessmentItemID']], on = ['big_features', 'assessmentItemID'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['assess_mean'] = temp['answer_rate']\n",
    "        df['assess_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path, parse_dates=['Timestamp'])#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "                \n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        self.args.n_big_features = 9\n",
    "\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag', 'big_features', 'answer_delta', 'tag_delta', 'test_delta', 'assess_delta', 'big_mean', 'tag_mean', 'test_mean', 'assess_mean', 'prior_elapsed', 'mean_elapsed', 'test_time', 'grade_time', 'tag_cumAnswer']\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['big_features'].values,\n",
    "                    r['answer_delta'].values,\n",
    "                    r['tag_delta'].values,\n",
    "                    r['test_delta'].values,\n",
    "                    r['assess_delta'].values,\n",
    "                    r['big_mean'].values,\n",
    "                    r['tag_mean'].values,\n",
    "                    r['test_mean'].values,\n",
    "                    r['assess_mean'].values,\n",
    "                    r['prior_elapsed'].values,\n",
    "                    r['mean_elapsed'].values,\n",
    "                    r['test_time'].values,\n",
    "                    r['grade_time'].values,\n",
    "                    r['tag_cumAnswer'].values,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "def slidding_window(data, args):\n",
    "    window_size = args.max_seq_len\n",
    "    stride = args.stride\n",
    "\n",
    "    augmented_datas = []\n",
    "    for row in data:\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        # 만약 window 크기보다 seq len이 같거나 작으면 augmentation을 하지 않는다\n",
    "        if seq_len <= window_size:\n",
    "            augmented_datas.append(row)\n",
    "        else:\n",
    "            total_window = ((seq_len - window_size) // stride) + 1\n",
    "            \n",
    "            # 앞에서부터 slidding window 적용\n",
    "            for window_i in range(total_window):\n",
    "                # window로 잘린 데이터를 모으는 리스트\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[window_i*stride:window_i*stride + window_size])\n",
    "\n",
    "                # Shuffle\n",
    "                # 마지막 데이터의 경우 shuffle을 하지 않는다\n",
    "                if args.shuffle and window_i + 1 != total_window:\n",
    "                    shuffle_datas = shuffle(window_data, window_size, args)\n",
    "                    augmented_datas += shuffle_datas\n",
    "                else:\n",
    "                    augmented_datas.append(tuple(window_data))\n",
    "\n",
    "            # slidding window에서 뒷부분이 누락될 경우 추가\n",
    "            total_len = window_size + (stride * (total_window - 1))\n",
    "            if seq_len != total_len:\n",
    "                window_data = []\n",
    "                for col in row:\n",
    "                    window_data.append(col[-window_size:])\n",
    "                augmented_datas.append(tuple(window_data))\n",
    "\n",
    "\n",
    "    return augmented_datas\n",
    "\n",
    "\n",
    "def shuffle(data, data_size, args):\n",
    "    shuffle_datas = []\n",
    "    for i in range(args.shuffle_n):\n",
    "        # shuffle 횟수만큼 window를 랜덤하게 계속 섞어서 데이터로 추가\n",
    "        shuffle_data = []\n",
    "        random_index = np.random.permutation(data_size)\n",
    "        for col in data:\n",
    "            shuffle_data.append(col[random_index])\n",
    "        shuffle_datas.append(tuple(shuffle_data))\n",
    "    return shuffle_datas\n",
    "\n",
    "def data_augmentation(data, args):\n",
    "    if args.window == True:\n",
    "        data = slidding_window(data, args)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(args.file_name)\n",
    "train_data = preprocess.get_train_data()\n",
    "\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b281f95-809a-48fe-b797-a2c4eb4607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct, big_features, answer_delta, tag_delta, test_delta, assess_delta, big_mean, tag_mean, test_mean, assess_mean, prior_elapsed, mean_elapsed, test_time, grade_time, tag_cumAnswer = row\n",
    "        \n",
    "\n",
    "        cate_cols = [test, question, tag, correct, big_features]\n",
    "        cont_cols = [answer_delta, tag_delta, test_delta, assess_delta, big_mean, tag_mean, test_mean, assess_mean, prior_elapsed, mean_elapsed, test_time, grade_time, tag_cumAnswer]\n",
    "        \n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols, cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    cate_col_n = len(batch[0][0])\n",
    "    cont_col_n = len(batch[0][1])\n",
    "\n",
    "    cate_col_list = [[] for _ in range(cate_col_n)]\n",
    "    cont_col_list = [[] for _ in range(cont_col_n)]\n",
    "\n",
    "    max_seq_len = len(batch[0][0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row[0]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cate_col_list[i].append(pre_padded)\n",
    "        for i, col in enumerate(row[1]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cont_col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(cate_col_list):\n",
    "        cate_col_list[i] =torch.stack(cate_col_list[i])\n",
    "    \n",
    "    for i, _ in enumerate(cont_col_list):\n",
    "        cont_col_list[i] =torch.stack(cont_col_list[i])\n",
    "\n",
    "    return tuple(cate_col_list), tuple(cont_col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, args):\n",
    "\n",
    "    (test, question, tag, correct, big_features, mask), cont_features = batch    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "    big_features = big_features.type(torch.FloatTensor)\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    # interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction_mask = mask.roll(shifts=1, dims=1)\n",
    "    interaction_mask[:, 0] = 0\n",
    "    interaction = (interaction * interaction_mask).to(torch.int64)\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    big_features = (big_features * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    for i, cont_feature in enumerate(cont_features):\n",
    "        cont_feature = cont_feature.type(torch.FloatTensor)\n",
    "        if i < 4:\n",
    "            cont_feature = cont_feature.roll(shifts=1, dims=1)\n",
    "            cont_feature[:, 0] = 0 # set padding index to the first sequence\n",
    "            cont_feature = (cont_feature * interaction_mask).unsqueeze(-1)\n",
    "        else:\n",
    "            cont_feature = (cont_feature * mask).unsqueeze(-1)\n",
    "        temp.append(cont_feature)\n",
    "    \n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "    interaction = interaction.to(args.device)\n",
    "    big_features = big_features.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    cont_features = torch.cat(temp, dim=-1).to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, big_features, gather_index), cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = process_batch(batch, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_cate = 5\n",
    "args.n_cont = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    # loss = loss[:,-1]\n",
    "    # loss = loss[:, :-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "        if args.scheduler != 'plateau':\n",
    "            scheduler.step()\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        # 큰 카테고리 embedding 추가\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*5, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(self.args.n_cont, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # Bert config\n",
    "        self.config = GPT2Config( \n",
    "            3, # not used\n",
    "            n_embd=self.hidden_dim,\n",
    "            n_layer=self.args.n_layers,\n",
    "            n_head=self.args.n_heads,\n",
    "            n_positions=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = GPT2Model(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(p=args.drop_out)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features, _), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "\n",
    "        # cate변수와 cont변수를 concat해서 bert의 input에 넣어줌        \n",
    "        X = torch.cat([cate_embed, cont_embed], 2)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers.last_hidden_state\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.dropout(self.fc(out))\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.hidden_dim = 512\n",
    "args.optimizer = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.model = 'gpt2'\n",
    "\n",
    "if args.model == 'customlstm':\n",
    "    model = CustomLSTM(args)\n",
    "elif args.model == 'customlstmattn':\n",
    "    model = CustomLSTMATTN(args)\n",
    "elif args.model == 'custombert':\n",
    "    model = CustomBert(args)\n",
    "elif args.model == 'customxlm':\n",
    "    model = CustomXlmRoberta(args)\n",
    "elif args.model == 'customsaint':\n",
    "    model = CustomSaint(args)\n",
    "elif args.model == 'customlastquery':\n",
    "    model = CustomLastQuery(args)\n",
    "elif args.model == 'gpt2':\n",
    "    model = GPT2(args)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /args.scheduler = 'plateau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.6987665891647339\n",
      "Training steps: 50 Loss: 0.467035174369812\n",
      "TRAIN AUC : 0.731706516596211 ACC : 0.6512372013651877\n",
      "VALID AUC : 0.7932390521898596 ACC : 0.7059701492537314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "        \n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    \n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, scheduler, args)\n",
    "    \n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    lr = get_lr(optimizer)\n",
    "    ### TODO: model save or early stopping\n",
    "    # wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "    #             \"valid_auc\":auc, \"valid_acc\":acc, \"lr\":lr})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        # model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        # delete_model(args.model_dir)\n",
    "        # save_checkpoint({\n",
    "        #     'epoch': epoch + 1,\n",
    "        #     'state_dict': model_to_save.state_dict(),\n",
    "        #     },\n",
    "        #     args.model_dir, f'model_{epoch + 1}.pt',\n",
    "        # )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcha-no\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">golden-galaxy-110</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/cha-no/DKT\" target=\"_blank\">https://wandb.ai/cha-no/DKT</a><br/>\n                Run page: <a href=\"https://wandb.ai/cha-no/DKT/runs/18ci0vej\" target=\"_blank\">https://wandb.ai/cha-no/DKT/runs/18ci0vej</a><br/>\n                Run data is saved locally in <code>/opt/ml/wandb/run-20210609_080710-18ci0vej</code><br/><br/>\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "args.add_features = [\"bigfeature\", \"answer_delta\",\"tag_delta\",\"test_delta\",\"assess_delta\", \"big_mean\", 'tag_mean', 'test_mean', \"assess_mean\",'prior_elapsed', 'mean_elapsed', 'test_time', 'grade_time', 'tag_cumAnswer']\n",
    "name = 'fe9_maxseq128_hiddendim512_gpt2'\n",
    "\n",
    "args.model_dir = '/opt/ml/models/'\n",
    "args.model_dir = increment_path(os.path.join(args.model_dir, args.model))\n",
    "args.save_path = args.model_dir.split('/')[-1]\n",
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project='dkt', config=vars(args))\n",
    "wandb.run.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpt2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(args.model)\n",
    "\n",
    "if args.model == 'customlstm':\n",
    "    model = CustomLSTM(args)\n",
    "elif args.model == 'customlstmattn':\n",
    "    model = CustomLSTMATTN(args)\n",
    "elif args.model == 'custombert':\n",
    "    model = CustomBert(args)\n",
    "elif args.model == 'customxlm':\n",
    "    model = CustomXlmRoberta(args)\n",
    "elif args.model == 'customsaint':\n",
    "    model = CustomSaint(args)\n",
    "elif args.model == 'customlastquery':\n",
    "    model = CustomLastQuery(args)\n",
    "elif args.model == 'gpt2':\n",
    "    model = GPT2(args)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Augmentation applied. Train data 4688 -> 10601\n\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# augmentation\n",
    "augmented_train_data = data_augmentation(train_data, args)\n",
    "if len(augmented_train_data) != len(train_data):\n",
    "    print(f\"Data Augmentation applied. Train data {len(train_data)} -> {len(augmented_train_data)}\\n\")\n",
    "\n",
    "train_loader, valid_loader = get_loaders(args, augmented_train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.647724986076355\n",
      "Training steps: 50 Loss: 0.459611177444458\n",
      "TRAIN AUC : 0.7335956466683138 ACC : 0.6548634812286689\n",
      "VALID AUC : 0.7953456495887474 ACC : 0.7044776119402985\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.4702879786491394\n",
      "Training steps: 50 Loss: 0.48483526706695557\n",
      "TRAIN AUC : 0.7587215689206104 ACC : 0.6708617747440273\n",
      "VALID AUC : 0.7978906255423943 ACC : 0.7243781094527363\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.4561827480792999\n",
      "Training steps: 50 Loss: 0.44875749945640564\n",
      "TRAIN AUC : 0.756398126732713 ACC : 0.66339590443686\n",
      "VALID AUC : 0.8027752734658982 ACC : 0.7233830845771144\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.4813230037689209\n",
      "Training steps: 50 Loss: 0.4687924087047577\n",
      "TRAIN AUC : 0.7591796752140337 ACC : 0.6781143344709898\n",
      "VALID AUC : 0.8016049415776272 ACC : 0.7169154228855721\n",
      "\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.43265852332115173\n",
      "Training steps: 50 Loss: 0.47385263442993164\n",
      "TRAIN AUC : 0.7587086234293335 ACC : 0.6751279863481229\n",
      "VALID AUC : 0.8020373692922764 ACC : 0.7189054726368159\n",
      "\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.47819197177886963\n",
      "Training steps: 50 Loss: 0.4811307191848755\n",
      "TRAIN AUC : 0.7626125095381832 ACC : 0.6793941979522184\n",
      "VALID AUC : 0.8019441394638889 ACC : 0.7199004975124378\n",
      "\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.46206796169281006\n",
      "Training steps: 50 Loss: 0.45749911665916443\n",
      "TRAIN AUC : 0.7605616519905516 ACC : 0.6732081911262798\n",
      "VALID AUC : 0.8027574209455687 ACC : 0.7263681592039801\n",
      "\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.4759540557861328\n",
      "Training steps: 50 Loss: 0.45158153772354126\n",
      "TRAIN AUC : 0.7547972617550986 ACC : 0.6674488054607508\n",
      "VALID AUC : 0.8030549629510613 ACC : 0.7293532338308458\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.46060121059417725\n",
      "Training steps: 50 Loss: 0.43553054332733154\n",
      "TRAIN AUC : 0.7667367424950338 ACC : 0.6811006825938567\n",
      "VALID AUC : 0.8035647515871386 ACC : 0.7213930348258707\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.47549593448638916\n",
      "Training steps: 50 Loss: 0.4622730612754822\n",
      "TRAIN AUC : 0.7677502285973195 ACC : 0.6753412969283277\n",
      "VALID AUC : 0.8040695811897911 ACC : 0.7238805970149254\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.4596993923187256\n",
      "Training steps: 50 Loss: 0.4600144028663635\n",
      "TRAIN AUC : 0.7629749832939348 ACC : 0.6749146757679181\n",
      "VALID AUC : 0.8039108921201951 ACC : 0.7253731343283583\n",
      "\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.44384366273880005\n",
      "Training steps: 50 Loss: 0.45885658264160156\n",
      "TRAIN AUC : 0.7680627436965941 ACC : 0.6808873720136519\n",
      "VALID AUC : 0.8042818278203758 ACC : 0.7203980099502487\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.43191877007484436\n",
      "Training steps: 50 Loss: 0.4433633089065552\n",
      "TRAIN AUC : 0.7636630999711006 ACC : 0.674061433447099\n",
      "VALID AUC : 0.8045476320119493 ACC : 0.7258706467661692\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.4322013556957245\n",
      "Training steps: 50 Loss: 0.45063525438308716\n",
      "TRAIN AUC : 0.7694253022361968 ACC : 0.6823805460750854\n",
      "VALID AUC : 0.8058131773419779 ACC : 0.7154228855721393\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.4697016775608063\n",
      "Training steps: 50 Loss: 0.4635443091392517\n",
      "TRAIN AUC : 0.7671398760332462 ACC : 0.6766211604095563\n",
      "VALID AUC : 0.8053549626535195 ACC : 0.7194029850746269\n",
      "\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.4557591676712036\n",
      "Training steps: 50 Loss: 0.4309083819389343\n",
      "TRAIN AUC : 0.7688502306941244 ACC : 0.6819539249146758\n",
      "VALID AUC : 0.8055890290311735 ACC : 0.7139303482587065\n",
      "\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.4621589183807373\n",
      "Training steps: 50 Loss: 0.419034481048584\n",
      "TRAIN AUC : 0.7734624722059391 ACC : 0.6843003412969283\n",
      "VALID AUC : 0.8058816120032413 ACC : 0.7303482587064677\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.4649665057659149\n",
      "Training steps: 50 Loss: 0.4652668833732605\n",
      "TRAIN AUC : 0.770630691571847 ACC : 0.6802474402730375\n",
      "VALID AUC : 0.8042649671067313 ACC : 0.7233830845771144\n",
      "\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.4558376669883728\n",
      "Training steps: 50 Loss: 0.45922401547431946\n",
      "TRAIN AUC : 0.7639984064282569 ACC : 0.6849402730375427\n",
      "VALID AUC : 0.8046210257066375 ACC : 0.7218905472636816\n",
      "\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.4401082396507263\n",
      "Training steps: 50 Loss: 0.46102529764175415\n",
      "TRAIN AUC : 0.7700652835655164 ACC : 0.6781143344709898\n",
      "VALID AUC : 0.8057467262940845 ACC : 0.7238805970149254\n",
      "\n",
      "Start Training: Epoch 21\n",
      "Training steps: 0 Loss: 0.4701823890209198\n",
      "Training steps: 50 Loss: 0.4801598787307739\n",
      "TRAIN AUC : 0.7762510404254873 ACC : 0.685580204778157\n",
      "VALID AUC : 0.8061960147223785 ACC : 0.7208955223880597\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 22\n",
      "Training steps: 0 Loss: 0.48219016194343567\n",
      "Training steps: 50 Loss: 0.4440114498138428\n",
      "TRAIN AUC : 0.768085899716202 ACC : 0.6770477815699659\n",
      "VALID AUC : 0.8073514695103748 ACC : 0.7293532338308458\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 23\n",
      "Training steps: 0 Loss: 0.47674983739852905\n",
      "Training steps: 50 Loss: 0.4273473620414734\n",
      "TRAIN AUC : 0.7707276004255602 ACC : 0.6849402730375427\n",
      "VALID AUC : 0.8062951953908759 ACC : 0.7218905472636816\n",
      "\n",
      "Start Training: Epoch 24\n",
      "Training steps: 0 Loss: 0.46627742052078247\n",
      "Training steps: 50 Loss: 0.4551423192024231\n",
      "TRAIN AUC : 0.770475528007388 ACC : 0.675981228668942\n",
      "VALID AUC : 0.8069597058698095 ACC : 0.7298507462686568\n",
      "\n",
      "Start Training: Epoch 25\n",
      "Training steps: 0 Loss: 0.43307191133499146\n",
      "Training steps: 50 Loss: 0.41975921392440796\n",
      "TRAIN AUC : 0.7731627202670782 ACC : 0.6877133105802048\n",
      "VALID AUC : 0.8068277955807077 ACC : 0.7308457711442786\n",
      "\n",
      "Start Training: Epoch 26\n",
      "Training steps: 0 Loss: 0.46258383989334106\n",
      "Training steps: 50 Loss: 0.4542374014854431\n",
      "TRAIN AUC : 0.7720284399680192 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.8078275367191631 ACC : 0.7253731343283583\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 27\n",
      "Training steps: 0 Loss: 0.43326815962791443\n",
      "Training steps: 50 Loss: 0.4663068652153015\n",
      "TRAIN AUC : 0.7718647068529967 ACC : 0.6791808873720137\n",
      "VALID AUC : 0.8074863552195315 ACC : 0.7298507462686568\n",
      "\n",
      "Start Training: Epoch 28\n",
      "Training steps: 0 Loss: 0.4391782283782959\n",
      "Training steps: 50 Loss: 0.46345287561416626\n",
      "TRAIN AUC : 0.7703238287293269 ACC : 0.6872866894197952\n",
      "VALID AUC : 0.8091952381377441 ACC : 0.7313432835820896\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 29\n",
      "Training steps: 0 Loss: 0.46987348794937134\n",
      "Training steps: 50 Loss: 0.45165959000587463\n",
      "TRAIN AUC : 0.7698927985690673 ACC : 0.6789675767918089\n",
      "VALID AUC : 0.8085376703056053 ACC : 0.7318407960199005\n",
      "\n",
      "Start Training: Epoch 30\n",
      "Training steps: 0 Loss: 0.46822017431259155\n",
      "Training steps: 50 Loss: 0.4592140316963196\n",
      "TRAIN AUC : 0.7735643039929548 ACC : 0.6834470989761092\n",
      "VALID AUC : 0.8090117539010236 ACC : 0.7308457711442786\n",
      "\n",
      "Start Training: Epoch 31\n",
      "Training steps: 0 Loss: 0.4511294364929199\n",
      "Training steps: 50 Loss: 0.454837441444397\n",
      "TRAIN AUC : 0.7682095200413526 ACC : 0.6768344709897611\n",
      "VALID AUC : 0.8091625085171399 ACC : 0.7323383084577114\n",
      "\n",
      "Start Training: Epoch 32\n",
      "Training steps: 0 Loss: 0.45863157510757446\n",
      "Training steps: 50 Loss: 0.4700985550880432\n",
      "TRAIN AUC : 0.77063743781378 ACC : 0.6802474402730375\n",
      "VALID AUC : 0.8089453028531304 ACC : 0.7328358208955223\n",
      "\n",
      "Start Training: Epoch 33\n",
      "Training steps: 0 Loss: 0.4645153880119324\n",
      "Training steps: 50 Loss: 0.4306660294532776\n",
      "TRAIN AUC : 0.7742324554406164 ACC : 0.6896331058020477\n",
      "VALID AUC : 0.8098200763492786 ACC : 0.7293532338308458\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 34\n",
      "Training steps: 0 Loss: 0.4637258052825928\n",
      "Training steps: 50 Loss: 0.451876699924469\n",
      "TRAIN AUC : 0.7761220413398766 ACC : 0.6872866894197952\n",
      "VALID AUC : 0.8093360746870105 ACC : 0.7333333333333333\n",
      "\n",
      "Start Training: Epoch 35\n",
      "Training steps: 0 Loss: 0.4432656168937683\n",
      "Training steps: 50 Loss: 0.4168505370616913\n",
      "TRAIN AUC : 0.7734013913668161 ACC : 0.6838737201365188\n",
      "VALID AUC : 0.8085545310192501 ACC : 0.7318407960199005\n",
      "\n",
      "Start Training: Epoch 36\n",
      "Training steps: 0 Loss: 0.4801276624202728\n",
      "Training steps: 50 Loss: 0.46089956164360046\n",
      "TRAIN AUC : 0.774585630322352 ACC : 0.6843003412969283\n",
      "VALID AUC : 0.8080665621302422 ACC : 0.7303482587064677\n",
      "\n",
      "Start Training: Epoch 37\n",
      "Training steps: 0 Loss: 0.45920199155807495\n",
      "Training steps: 50 Loss: 0.43681490421295166\n",
      "TRAIN AUC : 0.775765766833469 ACC : 0.6825938566552902\n",
      "VALID AUC : 0.8093360746870106 ACC : 0.7293532338308458\n",
      "\n",
      "Start Training: Epoch 38\n",
      "Training steps: 0 Loss: 0.4527737498283386\n",
      "Training steps: 50 Loss: 0.434257447719574\n",
      "TRAIN AUC : 0.7694666913421102 ACC : 0.6791808873720137\n",
      "VALID AUC : 0.8091793692307845 ACC : 0.7343283582089553\n",
      "\n",
      "Start Training: Epoch 39\n",
      "Training steps: 0 Loss: 0.4555200934410095\n",
      "Training steps: 50 Loss: 0.4324987530708313\n",
      "TRAIN AUC : 0.7748181933381773 ACC : 0.6924061433447098\n",
      "VALID AUC : 0.8091674675505648 ACC : 0.7323383084577114\n",
      "\n",
      "Start Training: Epoch 40\n",
      "Training steps: 0 Loss: 0.463517963886261\n",
      "Training steps: 50 Loss: 0.4334228038787842\n",
      "TRAIN AUC : 0.768727157361563 ACC : 0.6768344709897611\n",
      "VALID AUC : 0.808269882500662 ACC : 0.7318407960199005\n",
      "\n",
      "Start Training: Epoch 41\n",
      "Training steps: 0 Loss: 0.43875372409820557\n",
      "Training steps: 50 Loss: 0.43772757053375244\n",
      "TRAIN AUC : 0.7781724430603506 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.8085823016064293 ACC : 0.7293532338308458\n",
      "\n",
      "Start Training: Epoch 42\n",
      "Training steps: 0 Loss: 0.4510740339756012\n",
      "Training steps: 50 Loss: 0.4405669569969177\n",
      "TRAIN AUC : 0.770870091995037 ACC : 0.6860068259385665\n",
      "VALID AUC : 0.8091892872976343 ACC : 0.7378109452736319\n",
      "\n",
      "Start Training: Epoch 43\n",
      "Training steps: 0 Loss: 0.4499882757663727\n",
      "Training steps: 50 Loss: 0.4667128622531891\n",
      "TRAIN AUC : 0.7749276830214411 ACC : 0.6834470989761092\n",
      "VALID AUC : 0.8097060185805064 ACC : 0.7353233830845771\n",
      "\n",
      "Start Training: Epoch 44\n",
      "Training steps: 0 Loss: 0.43115419149398804\n",
      "Training steps: 50 Loss: 0.45507872104644775\n",
      "TRAIN AUC : 0.7777952005046919 ACC : 0.6843003412969283\n",
      "VALID AUC : 0.8094174028351787 ACC : 0.7348258706467662\n",
      "\n",
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Start Training: Epoch 45\n",
      "Training steps: 0 Loss: 0.4263634979724884\n",
      "Training steps: 50 Loss: 0.44469931721687317\n",
      "TRAIN AUC : 0.7774585265660626 ACC : 0.683660409556314\n",
      "VALID AUC : 0.8094858374964419 ACC : 0.7398009950248756\n",
      "\n",
      "Start Training: Epoch 46\n",
      "Training steps: 0 Loss: 0.442929744720459\n",
      "Training steps: 50 Loss: 0.4396483302116394\n",
      "TRAIN AUC : 0.7688511423484399 ACC : 0.6725682593856656\n",
      "VALID AUC : 0.8093311156535858 ACC : 0.7338308457711443\n",
      "\n",
      "Start Training: Epoch 47\n",
      "Training steps: 0 Loss: 0.44171661138534546\n",
      "Training steps: 50 Loss: 0.46075692772865295\n",
      "TRAIN AUC : 0.7809043975469205 ACC : 0.6896331058020477\n",
      "VALID AUC : 0.8091287870898507 ACC : 0.7338308457711443\n",
      "\n",
      "Start Training: Epoch 48\n",
      "Training steps: 0 Loss: 0.4291839897632599\n",
      "Training steps: 50 Loss: 0.4447128176689148\n",
      "TRAIN AUC : 0.7773918846356161 ACC : 0.6864334470989761\n",
      "VALID AUC : 0.8103174674017938 ACC : 0.7388059701492538\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 49\n",
      "Training steps: 0 Loss: 0.44775494933128357\n",
      "Training steps: 50 Loss: 0.4508398771286011\n",
      "TRAIN AUC : 0.7810917425087086 ACC : 0.6832337883959044\n",
      "VALID AUC : 0.8090177047411335 ACC : 0.7308457711442786\n",
      "\n",
      "Start Training: Epoch 50\n",
      "Training steps: 0 Loss: 0.4234244227409363\n",
      "Training steps: 50 Loss: 0.45569127798080444\n",
      "TRAIN AUC : 0.778767753328222 ACC : 0.6857935153583617\n",
      "VALID AUC : 0.8098369370629233 ACC : 0.7353233830845771\n",
      "\n",
      "Start Training: Epoch 51\n",
      "Training steps: 0 Loss: 0.4779452085494995\n",
      "Training steps: 50 Loss: 0.4470398426055908\n",
      "TRAIN AUC : 0.7762786635512401 ACC : 0.6838737201365188\n",
      "VALID AUC : 0.8092914433861866 ACC : 0.7338308457711443\n",
      "\n",
      "Start Training: Epoch 52\n",
      "Training steps: 0 Loss: 0.4406772255897522\n",
      "Training steps: 50 Loss: 0.44930940866470337\n",
      "TRAIN AUC : 0.7713001193355499 ACC : 0.6755546075085325\n",
      "VALID AUC : 0.8090563852018475 ACC : 0.7373134328358208\n",
      "\n",
      "Start Training: Epoch 53\n",
      "Training steps: 0 Loss: 0.4439930021762848\n",
      "Training steps: 50 Loss: 0.4307096600532532\n",
      "TRAIN AUC : 0.7753800458926781 ACC : 0.6857935153583617\n",
      "VALID AUC : 0.8093043368730913 ACC : 0.735820895522388\n",
      "\n",
      "Start Training: Epoch 54\n",
      "Training steps: 0 Loss: 0.4507825970649719\n",
      "Training steps: 50 Loss: 0.4416199028491974\n",
      "TRAIN AUC : 0.784812386100189 ACC : 0.6924061433447098\n",
      "VALID AUC : 0.8096365921125581 ACC : 0.7373134328358208\n",
      "\n",
      "Start Training: Epoch 55\n",
      "Training steps: 0 Loss: 0.4614196717739105\n",
      "Training steps: 50 Loss: 0.45864105224609375\n",
      "TRAIN AUC : 0.7772131092243919 ACC : 0.6845136518771331\n",
      "VALID AUC : 0.8102078727631039 ACC : 0.7333333333333333\n",
      "\n",
      "Start Training: Epoch 56\n",
      "Training steps: 0 Loss: 0.4535894989967346\n",
      "Training steps: 50 Loss: 0.4597908556461334\n",
      "TRAIN AUC : 0.7773684551197139 ACC : 0.6958191126279863\n",
      "VALID AUC : 0.809036549068148 ACC : 0.7422885572139304\n",
      "\n",
      "Start Training: Epoch 57\n",
      "Training steps: 0 Loss: 0.4572139084339142\n",
      "Training steps: 50 Loss: 0.44023293256759644\n",
      "TRAIN AUC : 0.7783700897159012 ACC : 0.6887798634812287\n",
      "VALID AUC : 0.8097595761414952 ACC : 0.7328358208955223\n",
      "\n",
      "Start Training: Epoch 58\n",
      "Training steps: 0 Loss: 0.45543479919433594\n",
      "Training steps: 50 Loss: 0.4592939019203186\n",
      "TRAIN AUC : 0.7770086251614768 ACC : 0.6879266211604096\n",
      "VALID AUC : 0.8101513397820604 ACC : 0.7378109452736319\n",
      "\n",
      "Start Training: Epoch 59\n",
      "Training steps: 0 Loss: 0.45104748010635376\n",
      "Training steps: 50 Loss: 0.43247267603874207\n",
      "TRAIN AUC : 0.7722621881344545 ACC : 0.6821672354948806\n",
      "VALID AUC : 0.8095527844476778 ACC : 0.7333333333333333\n",
      "\n",
      "Epoch    59: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Start Training: Epoch 60\n",
      "Training steps: 0 Loss: 0.44185078144073486\n",
      "Training steps: 50 Loss: 0.4383944869041443\n",
      "TRAIN AUC : 0.7747873794223211 ACC : 0.685580204778157\n",
      "VALID AUC : 0.8103655700260151 ACC : 0.7388059701492538\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 61\n",
      "Training steps: 0 Loss: 0.41816937923431396\n",
      "Training steps: 50 Loss: 0.45210325717926025\n",
      "TRAIN AUC : 0.7750593259045663 ACC : 0.683660409556314\n",
      "VALID AUC : 0.8096475019860928 ACC : 0.7323383084577114\n",
      "\n",
      "Start Training: Epoch 62\n",
      "Training steps: 0 Loss: 0.4650872051715851\n",
      "Training steps: 50 Loss: 0.4409332871437073\n",
      "TRAIN AUC : 0.779832656733889 ACC : 0.6904863481228669\n",
      "VALID AUC : 0.8102187826366388 ACC : 0.7378109452736319\n",
      "\n",
      "Start Training: Epoch 63\n",
      "Training steps: 0 Loss: 0.4582862854003906\n",
      "Training steps: 50 Loss: 0.46224236488342285\n",
      "TRAIN AUC : 0.7776578141993807 ACC : 0.695179180887372\n",
      "VALID AUC : 0.8095582393844449 ACC : 0.7383084577114428\n",
      "\n",
      "Start Training: Epoch 64\n",
      "Training steps: 0 Loss: 0.4291421175003052\n",
      "Training steps: 50 Loss: 0.4463921785354614\n",
      "TRAIN AUC : 0.7714762509492601 ACC : 0.6832337883959044\n",
      "VALID AUC : 0.8096465101794079 ACC : 0.7388059701492538\n",
      "\n",
      "Start Training: Epoch 65\n",
      "Training steps: 0 Loss: 0.4320339560508728\n",
      "Training steps: 50 Loss: 0.43444448709487915\n",
      "TRAIN AUC : 0.7793319761839427 ACC : 0.6872866894197952\n",
      "VALID AUC : 0.8109229653829713 ACC : 0.746268656716418\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 66\n",
      "Training steps: 0 Loss: 0.4502411484718323\n",
      "Training steps: 50 Loss: 0.45349764823913574\n",
      "TRAIN AUC : 0.7784546912363582 ACC : 0.6857935153583617\n",
      "VALID AUC : 0.8103695372527548 ACC : 0.7427860696517413\n",
      "\n",
      "Start Training: Epoch 67\n",
      "Training steps: 0 Loss: 0.4230861961841583\n",
      "Training steps: 50 Loss: 0.4189559817314148\n",
      "TRAIN AUC : 0.7750848522253938 ACC : 0.6900597269624573\n",
      "VALID AUC : 0.8106750137117275 ACC : 0.7427860696517413\n",
      "\n",
      "Start Training: Epoch 68\n",
      "Training steps: 0 Loss: 0.447599321603775\n",
      "Training steps: 50 Loss: 0.4864172041416168\n",
      "TRAIN AUC : 0.7792759094435535 ACC : 0.6870733788395904\n",
      "VALID AUC : 0.8097208956807811 ACC : 0.7398009950248756\n",
      "\n",
      "Start Training: Epoch 69\n",
      "Training steps: 0 Loss: 0.4324548840522766\n",
      "Training steps: 50 Loss: 0.46691107749938965\n",
      "TRAIN AUC : 0.7789780719787548 ACC : 0.6911262798634812\n",
      "VALID AUC : 0.8102346515435983 ACC : 0.7412935323383084\n",
      "\n",
      "Start Training: Epoch 70\n",
      "Training steps: 0 Loss: 0.45171698927879333\n",
      "Training steps: 50 Loss: 0.42913201451301575\n",
      "TRAIN AUC : 0.7759021503190334 ACC : 0.6900597269624573\n",
      "VALID AUC : 0.8105569887162154 ACC : 0.7412935323383084\n",
      "\n",
      "Start Training: Epoch 71\n",
      "Training steps: 0 Loss: 0.4282575249671936\n",
      "Training steps: 50 Loss: 0.47752267122268677\n",
      "TRAIN AUC : 0.7760070817307211 ACC : 0.6843003412969283\n",
      "VALID AUC : 0.8100997658344417 ACC : 0.7402985074626866\n",
      "\n",
      "Start Training: Epoch 72\n",
      "Training steps: 0 Loss: 0.42581707239151\n",
      "Training steps: 50 Loss: 0.43832263350486755\n",
      "TRAIN AUC : 0.7694469084434687 ACC : 0.6819539249146758\n",
      "VALID AUC : 0.8101582824288553 ACC : 0.7388059701492538\n",
      "\n",
      "Start Training: Epoch 73\n",
      "Training steps: 0 Loss: 0.40325605869293213\n",
      "Training steps: 50 Loss: 0.43302953243255615\n",
      "TRAIN AUC : 0.7746049573938356 ACC : 0.6881399317406144\n",
      "VALID AUC : 0.8102832500711622 ACC : 0.7412935323383084\n",
      "\n",
      "Start Training: Epoch 74\n",
      "Training steps: 0 Loss: 0.4496290683746338\n",
      "Training steps: 50 Loss: 0.4559771418571472\n",
      "TRAIN AUC : 0.7749781886705073 ACC : 0.6832337883959044\n",
      "VALID AUC : 0.8105227713855836 ACC : 0.7402985074626866\n",
      "\n",
      "Start Training: Epoch 75\n",
      "Training steps: 0 Loss: 0.4423672556877136\n",
      "Training steps: 50 Loss: 0.422483891248703\n",
      "TRAIN AUC : 0.7834266715409783 ACC : 0.6943259385665529\n",
      "VALID AUC : 0.8104290456538534 ACC : 0.7412935323383084\n",
      "\n",
      "Start Training: Epoch 76\n",
      "Training steps: 0 Loss: 0.4482903480529785\n",
      "Training steps: 50 Loss: 0.42540478706359863\n",
      "TRAIN AUC : 0.7831159797503344 ACC : 0.6917662116040956\n",
      "VALID AUC : 0.8105917019501896 ACC : 0.7398009950248756\n",
      "\n",
      "Epoch    76: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Start Training: Epoch 77\n",
      "Training steps: 0 Loss: 0.42049306631088257\n",
      "Training steps: 50 Loss: 0.42353272438049316\n",
      "TRAIN AUC : 0.7780993283842659 ACC : 0.685580204778157\n",
      "VALID AUC : 0.8100967904143868 ACC : 0.7388059701492538\n",
      "\n",
      "Start Training: Epoch 78\n",
      "Training steps: 0 Loss: 0.4428924024105072\n",
      "Training steps: 50 Loss: 0.4186103940010071\n",
      "TRAIN AUC : 0.7793368079518136 ACC : 0.6934726962457338\n",
      "VALID AUC : 0.8102376269636533 ACC : 0.7402985074626866\n",
      "\n",
      "Start Training: Epoch 79\n",
      "Training steps: 0 Loss: 0.44265127182006836\n",
      "Training steps: 50 Loss: 0.4147273004055023\n",
      "TRAIN AUC : 0.7812985057074118 ACC : 0.6889931740614335\n",
      "VALID AUC : 0.8106789809384674 ACC : 0.7417910447761195\n",
      "\n",
      "Start Training: Epoch 80\n",
      "Training steps: 0 Loss: 0.4854641556739807\n",
      "Training steps: 50 Loss: 0.44321560859680176\n",
      "TRAIN AUC : 0.782032022769478 ACC : 0.6902730375426621\n",
      "VALID AUC : 0.8102356433502833 ACC : 0.7393034825870647\n",
      "\n",
      "EarlyStopping counter: 15 out of 15\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 27453<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef73c1ee09af4f998459d7dd4ecbfa23"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/opt/ml/wandb/run-20210609_080710-18ci0vej/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/opt/ml/wandb/run-20210609_080710-18ci0vej/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>79</td></tr><tr><td>train_loss</td><td>0.44601</td></tr><tr><td>train_auc</td><td>0.78203</td></tr><tr><td>train_acc</td><td>0.69027</td></tr><tr><td>valid_auc</td><td>0.81024</td></tr><tr><td>valid_acc</td><td>0.7393</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>_runtime</td><td>806</td></tr><tr><td>_timestamp</td><td>1623226836</td></tr><tr><td>_step</td><td>79</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▁▄▅▅▆▅▅▆▇▅▇▆▇▆▆▆▇▇▇▇▇▇▇██▇▇▇▇▆▇▇▇▇▇▇▇█▇█</td></tr><tr><td>train_acc</td><td>▁▂▅▄▆▄▄▅▆▆▆▆▇▅▅▅▇▆▆█▆▆▆▇▆▆▆▆▇▆▆█▇▇▇▆▇█▆▇</td></tr><tr><td>valid_auc</td><td>▁▄▄▄▅▅▅▅▆▅▆▆▆▆▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇████████</td></tr><tr><td>valid_acc</td><td>▁▄▃▅▄▅▅▃▅▄▄▄▅▅▆▆▅▆▅▆▅▆▇▆▅▆▆▆▆▆▆▇█▇▇▇▇▇▇▇</td></tr><tr><td>lr</td><td>██████████████████████▄▄▄▄▄▄▄▄▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">golden-galaxy-110</strong>: <a href=\"https://wandb.ai/cha-no/DKT/runs/18ci0vej\" target=\"_blank\">https://wandb.ai/cha-no/DKT/runs/18ci0vej</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "        \n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    \n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, scheduler, args)\n",
    "    \n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    lr = get_lr(optimizer)\n",
    "    ### TODO: model save or early stopping\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                \"valid_auc\":auc, \"valid_acc\":acc, \"lr\":lr})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        delete_model(args.model_dir)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model_to_save.state_dict(),\n",
    "            },\n",
    "            args.model_dir, f'model_{epoch + 1}.pt',\n",
    "        )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(args.test_file_name)\n",
    "test_data = preprocess.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_loader = get_loaders(args, None, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break\n",
    "\n",
    "input = process_batch(batch, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):    \n",
    "    model_path = args.model_dir\n",
    "    print(\"Loading Model from:\", args.model_dir)\n",
    "    load_state = torch.load(model_path)\n",
    "\n",
    "    if args.model == 'customlstm':\n",
    "        model = CustomLSTM(args)\n",
    "    elif args.model == 'customlstmattn':\n",
    "        model = CustomLSTMATTN(args)\n",
    "    elif args.model == 'custombert':\n",
    "        model = CustomBert(args)\n",
    "    elif args.model == 'customxlm':\n",
    "        model = CustomXlmRoberta(args)\n",
    "    elif args.model == 'customsaint':\n",
    "        model = CustomSaint(args)\n",
    "    elif args.model == 'customlastquery':\n",
    "        model = CustomLastQuery(args)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "    \n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model from: /opt/ml/models/custombert15/model_41.pt\n",
      "Loading Model from: /opt/ml/models/custombert15/model_41.pt ...Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.model_dir = '/opt/ml/models'\n",
    "args.model = 'custombert'\n",
    "args.model_name = 'custombert15/model_41.pt'\n",
    "args.model_dir = os.path.join(args.model_dir, args.model_name)\n",
    "model = load_model(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args, test_data):\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "    \n",
    "    output_path = os.path.dirname(args.model_name)\n",
    "    os.makedirs(os.path.join(args.output_dir, output_path), exist_ok=True)\n",
    "    write_path = os.path.join(args.output_dir, output_path, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    print(write_path)\n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/output/custombert15/output.csv\nwriting prediction : /opt/ml/output/custombert15/output.csv\n"
     ]
    }
   ],
   "source": [
    "inference(args, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}