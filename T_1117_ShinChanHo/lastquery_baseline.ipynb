{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa65a72-1bf1-44f7-9da7-6a21914459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "from transformers import XLMRobertaModel, XLMRobertaConfig \n",
    "\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "from dkt.utils import setSeeds, increment_path, delete_model\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.trainer import compute_loss, update_params, get_lr, save_checkpoint\n",
    "from dkt.metric import get_metric\n",
    "from dkt.criterion import get_criterion\n",
    "from dkt.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa52652-164e-47e6-bcf3-e1f11c3e85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = {\n",
    "    'seed' : 42,\n",
    "    'device' : 'cuda',\n",
    "    'data_dir' : '/opt/ml/input/data/train_dataset',\n",
    "    'asset_dir' : '/opt/ml/asset/',\n",
    "    'file_name' : 'train_data.csv',\n",
    "    'model_dir' : '/opt/ml/models/',\n",
    "    'model_name' : 'model.pt',\n",
    "    'output_dir' : '/opt/ml/output/',\n",
    "    'test_file_name' : 'test_data.csv',\n",
    "    'max_seq_len' : 128,\n",
    "    'num_workers' : 1,\n",
    "    'hidden_dim' : 256,\n",
    "    'n_layers' : 2,\n",
    "    'n_heads' : 2,\n",
    "    'drop_out' : 0.2,\n",
    "    'n_epochs' : 200,\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.0001,\n",
    "    'clip_grad' : 10,\n",
    "    'patience' : 15,\n",
    "    'log_steps' : 50,\n",
    "    'model' : 'lstm',\n",
    "    'optimizer' : 'adam',\n",
    "    'scheduler' : 'plateau'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**namespace)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.device = device\n",
    "setSeeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb675f0e-7713-4a65-8882-5bf887908de9",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9098a1c4-d70a-4a19-82de-fe7bcdaa9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if str(x) in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        def convert_time(s):\n",
    "            timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "            return int(timestamp)\n",
    "\n",
    "        df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        #TODO\n",
    "        def percentile(s):\n",
    "            return np.sum(s) / len(s)\n",
    "\n",
    "        df = df.reset_index()\n",
    "        # 큰 카테고리\n",
    "        df['big_features'] = df['testId'].apply(lambda x : x[2]).astype(int)\n",
    "\n",
    "        # 큰 카테고리별 정답률\n",
    "        stu_groupby = df.groupby('big_features').agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # tag별 정답률\n",
    "        stu_tag_groupby = df.groupby(['big_features', 'KnowledgeTag']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 시험지별 정답률\n",
    "        stu_test_groupby = df.groupby(['big_features', 'testId']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 문항별 정답률\n",
    "        stu_assessment_groupby = df.groupby(['big_features', 'assessmentItemID']).agg({\n",
    "        'assessmentItemID': 'count',\n",
    "        'answerCode': percentile\n",
    "        }).rename(columns = {'assessmentItemID' : 'assessment_count', 'answerCode' : 'answer_rate'})\n",
    "\n",
    "        df = df.sort_values(by='index', axis=0)\n",
    "\n",
    "        # 정답 - 큰 카테고리별 정답률 \n",
    "        '''ex)\n",
    "        맞은 문제의 큰 카테고리별 정답률이 0.7 이면 1 - 0.7 = 0.3이 됨)\n",
    "        틀린 문제의 큰 카테고리별 정답률이 0.7 이면 0 - 0.7 = -0.7이 됨)\n",
    "        '''\n",
    "        temp = pd.merge(df, stu_groupby.reset_index()[['big_features', 'answer_rate']], on = ['big_features'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['big_mean'] = temp['answer_rate']\n",
    "        df['answer_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 태그별 정답률\n",
    "        temp = pd.merge(df, stu_tag_groupby.reset_index()[['answer_rate', 'big_features', 'KnowledgeTag']], on = ['big_features', 'KnowledgeTag'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['tag_mean'] = temp['answer_rate']\n",
    "        df['tag_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 시험별 정답률\n",
    "        temp = pd.merge(df, stu_test_groupby.reset_index()[['answer_rate', 'big_features', 'testId']], on = ['big_features', 'testId'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['test_mean'] = temp['answer_rate']\n",
    "        df['test_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 문항별 정답률\n",
    "        temp = pd.merge(df, stu_assessment_groupby.reset_index()[['answer_rate', 'big_features', 'assessmentItemID']], on = ['big_features', 'assessmentItemID'])\n",
    "        temp = temp.sort_values(by='index', axis=0).set_index('index')\n",
    "        df['assess_mean'] = temp['answer_rate']\n",
    "        df['assess_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "                \n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        self.args.n_big_features = 9\n",
    "\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag', 'big_features', 'answer_delta', 'tag_delta', 'test_delta', 'assess_delta', 'big_mean', 'tag_mean', 'test_mean', 'assess_mean']\n",
    "        # columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag', 'big_features', 'answer_delta', 'tag_delta', 'test_delta', 'assess_delta']\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['big_features'].values,\n",
    "                    r['answer_delta'].values,\n",
    "                    r['tag_delta'].values,\n",
    "                    r['test_delta'].values,\n",
    "                    r['assess_delta'].values,\n",
    "                    r['big_mean'].values,\n",
    "                    r['tag_mean'].values,\n",
    "                    r['test_mean'].values,\n",
    "                    r['assess_mean'].values,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(args.file_name)\n",
    "train_data = preprocess.get_train_data()\n",
    "\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b281f95-809a-48fe-b797-a2c4eb4607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct, big_features, answer_delta, tag_delta, test_delta, assess_delta, big_mean, tag_mean, test_mean, assess_mean = row\n",
    "        \n",
    "        cate_cols = [test, question, tag, correct, big_features]\n",
    "        cont_cols = [answer_delta, tag_delta, test_delta, assess_delta, big_mean, tag_mean, test_mean, assess_mean]\n",
    "        \n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols, cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    cate_col_n = len(batch[0][0])\n",
    "    cont_col_n = len(batch[0][1])\n",
    "\n",
    "    cate_col_list = [[] for _ in range(cate_col_n)]\n",
    "    cont_col_list = [[] for _ in range(cont_col_n)]\n",
    "\n",
    "    max_seq_len = len(batch[0][0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row[0]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cate_col_list[i].append(pre_padded)\n",
    "        for i, col in enumerate(row[1]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cont_col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(cate_col_list):\n",
    "        cate_col_list[i] =torch.stack(cate_col_list[i])\n",
    "    \n",
    "    for i, _ in enumerate(cont_col_list):\n",
    "        cont_col_list[i] =torch.stack(cont_col_list[i])\n",
    "\n",
    "    return tuple(cate_col_list), tuple(cont_col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, args):\n",
    "\n",
    "    (test, question, tag, correct, big_features, mask), cont_features = batch    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "    big_features = big_features.type(torch.FloatTensor)\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    # interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction_mask = mask.roll(shifts=1, dims=1)\n",
    "    interaction_mask[:, 0] = 0\n",
    "    interaction = (interaction * interaction_mask).to(torch.int64)\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    big_features = (big_features * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    for i, cont_feature in enumerate(cont_features):\n",
    "        cont_feature = cont_feature.type(torch.FloatTensor)\n",
    "        if i < 4:\n",
    "            cont_feature = cont_feature.roll(shifts=1, dims=1)\n",
    "            cont_feature[:, 0] = 0 # set padding index to the first sequence\n",
    "            cont_feature = (cont_feature * interaction_mask).unsqueeze(-1)\n",
    "        else:\n",
    "            cont_feature = (cont_feature * mask).unsqueeze(-1)\n",
    "        temp.append(cont_feature)\n",
    "    \n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "    interaction = interaction.to(args.device)\n",
    "    big_features = big_features.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    cont_features = torch.cat(temp, dim=-1).to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, big_features, gather_index), cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_cate = 5\n",
    "args.n_cont = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "\n",
    "    def forward(self,ffn_in):\n",
    "        return self.layer2(F.relu(self.layer1(ffn_in)))\n",
    "\n",
    "class CustomLastQuery(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CustomLastQuery, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        \n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "\n",
    "        self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*self.args.n_cate, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(self.args.n_cont, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # 기존 keetar님 솔루션에서는 Positional Embedding은 사용되지 않습니다\n",
    "        # 하지만 사용 여부는 자유롭게 결정해주세요 :)\n",
    "        # self.embedding_position = nn.Embedding(self.args.max_seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.query = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.key = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.value = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=self.args.n_heads)\n",
    "        self.mask = None # last query에서는 필요가 없지만 수정을 고려하여서 넣어둠\n",
    "        self.ffn = Feed_Forward_block(self.hidden_dim)      \n",
    "\n",
    "        self.ln1 = nn.LayerNorm(self.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.hidden_dim,\n",
    "            self.hidden_dim,\n",
    "            self.args.n_layers,\n",
    "            batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def get_pos(self, seq_len):\n",
    "        # use sine positional embeddinds\n",
    "        return torch.arange(seq_len).unsqueeze(0)\n",
    " \n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.args.n_layers,\n",
    "            batch_size,\n",
    "            self.args.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.args.n_layers,\n",
    "            batch_size,\n",
    "            self.args.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features, index), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "        seq_len = interaction.size(1)\n",
    "\n",
    "        # Embedding\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "        \n",
    "        embed = torch.cat([cate_embed, cont_embed], 2)\n",
    "\n",
    "        # Positional Embedding\n",
    "        # last query에서는 positional embedding을 하지 않음\n",
    "        # position = self.get_pos(seq_len).to('cuda')\n",
    "        # embed_pos = self.embedding_position(position)\n",
    "        # embed = embed + embed_pos\n",
    "\n",
    "        ####################### ENCODER #####################\n",
    "        q = self.query(embed)[:, -1:, :].permute(1, 0, 2)\n",
    "        k = self.key(embed).permute(1, 0, 2)\n",
    "        v = self.value(embed).permute(1, 0, 2)\n",
    "\n",
    "        ## attention\n",
    "        # last query only\n",
    "        out, _ = self.attn(q, k, v)\n",
    "        \n",
    "        ## residual + layer norm\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = embed + out\n",
    "        out = self.ln1(out)\n",
    "\n",
    "        ## feed forward network\n",
    "        out = self.ffn(out)\n",
    "\n",
    "        ## residual + layer norm\n",
    "        out = embed + out\n",
    "        out = self.ln2(out)\n",
    "\n",
    "        ###################### LSTM #####################\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(out, hidden)\n",
    "\n",
    "        ###################### DNN #####################\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    # loss = loss[:,-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "        if args.scheduler != 'plateau':\n",
    "            scheduler.step()\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.model = 'customlastquery'\n",
    "\n",
    "if args.model == 'customlstm':\n",
    "    model = CustomLSTM(args)\n",
    "elif args.model == 'customlstmattn':\n",
    "    model = CustomLSTMATTN(args)\n",
    "elif args.model == 'custombert':\n",
    "    model = CustomBert(args)\n",
    "elif args.model == 'customxlm':\n",
    "    model = CustomXlmRoberta(args)\n",
    "elif args.model == 'customsaint':\n",
    "    model = CustomSaint(args)\n",
    "elif args.model == 'customlastquery':\n",
    "    model = CustomLastQuery(args)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.add_features = [\"bigfeature\", \"answer_delta\",\"tag_delta\",\"test_delta\",\"assess_delta\", \"big_mean\", 'tag_mean', 'test_mean', \"assess_mean\"]\n",
    "# name = 'fe6_maxseq64_hiddendim512_customlastquery'\n",
    "\n",
    "# args.model_dir = '/opt/ml/models/'\n",
    "# args.model_dir = increment_path(os.path.join(args.model_dir, args.model))\n",
    "# args.save_path = args.model_dir.split('/')[-1]\n",
    "# os.makedirs(args.model_dir, exist_ok=True)\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(project='dkt', config=vars(args))\n",
    "# wandb.run.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.6904147863388062\n",
      "Training steps: 50 Loss: 0.5320302844047546\n",
      "TRAIN AUC : 0.7239530789757016 ACC : 0.6113481228668942\n",
      "VALID AUC : 0.7358729535308813 ACC : 0.6626865671641791\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.5585399270057678\n",
      "Training steps: 50 Loss: 0.5115206837654114\n",
      "TRAIN AUC : 0.7446758020506752 ACC : 0.6638225255972696\n",
      "VALID AUC : 0.739665126390885 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.5431641340255737\n",
      "Training steps: 50 Loss: 0.535885751247406\n",
      "TRAIN AUC : 0.7464443202568677 ACC : 0.6668088737201365\n",
      "VALID AUC : 0.7461718741476662 ACC : 0.6666666666666666\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.523822009563446\n",
      "Training steps: 50 Loss: 0.5198385119438171\n",
      "TRAIN AUC : 0.7537035500730691 ACC : 0.6719283276450512\n",
      "VALID AUC : 0.75428981186419 ACC : 0.6845771144278607\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.5439143180847168\n",
      "Training steps: 50 Loss: 0.5338912010192871\n",
      "TRAIN AUC : 0.7607477206362983 ACC : 0.6811006825938567\n",
      "VALID AUC : 0.7670057653722598 ACC : 0.6930348258706468\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.5274717807769775\n",
      "Training steps: 50 Loss: 0.4962862730026245\n",
      "TRAIN AUC : 0.7779359599309695 ACC : 0.6949658703071673\n",
      "VALID AUC : 0.7911979140321801 ACC : 0.7054726368159204\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.4869391620159149\n",
      "Training steps: 50 Loss: 0.41980648040771484\n",
      "TRAIN AUC : 0.7950561898137217 ACC : 0.7158703071672355\n",
      "VALID AUC : 0.7990718673042 ACC : 0.718407960199005\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.4490432143211365\n",
      "Training steps: 50 Loss: 0.4652615785598755\n",
      "TRAIN AUC : 0.8001268111152542 ACC : 0.726962457337884\n",
      "VALID AUC : 0.8052537983716518 ACC : 0.7228855721393035\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.46960315108299255\n",
      "Training steps: 50 Loss: 0.42778152227401733\n",
      "TRAIN AUC : 0.8018285962255689 ACC : 0.73080204778157\n",
      "VALID AUC : 0.8045585418854839 ACC : 0.7378109452736319\n",
      "\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.465657114982605\n",
      "Training steps: 50 Loss: 0.43749576807022095\n",
      "TRAIN AUC : 0.8017673330555827 ACC : 0.7293088737201365\n",
      "VALID AUC : 0.8057110212534254 ACC : 0.7318407960199005\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.4790124297142029\n",
      "Training steps: 50 Loss: 0.47113651037216187\n",
      "TRAIN AUC : 0.8025705005073356 ACC : 0.7297354948805461\n",
      "VALID AUC : 0.8058201199887727 ACC : 0.7348258706467662\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.4612906575202942\n",
      "Training steps: 50 Loss: 0.4674341380596161\n",
      "TRAIN AUC : 0.8034574489906618 ACC : 0.7322952218430034\n",
      "VALID AUC : 0.8055334878568148 ACC : 0.7353233830845771\n",
      "\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.4464587867259979\n",
      "Training steps: 50 Loss: 0.45907947421073914\n",
      "TRAIN AUC : 0.8039833823651412 ACC : 0.7314419795221843\n",
      "VALID AUC : 0.8074119697181582 ACC : 0.7308457711442786\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.4545132517814636\n",
      "Training steps: 50 Loss: 0.43692973256111145\n",
      "TRAIN AUC : 0.8044536136609576 ACC : 0.7331484641638225\n",
      "VALID AUC : 0.8046319355801722 ACC : 0.7293532338308458\n",
      "\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.4531369209289551\n",
      "Training steps: 50 Loss: 0.4674432873725891\n",
      "TRAIN AUC : 0.8054011871562493 ACC : 0.7320819112627986\n",
      "VALID AUC : 0.8071540999800645 ACC : 0.7313432835820896\n",
      "\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.4684039056301117\n",
      "Training steps: 50 Loss: 0.4643363952636719\n",
      "TRAIN AUC : 0.8060213855869276 ACC : 0.7361348122866894\n",
      "VALID AUC : 0.8069101155355606 ACC : 0.7298507462686568\n",
      "\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.4531271457672119\n",
      "Training steps: 50 Loss: 0.4251548647880554\n",
      "TRAIN AUC : 0.80396241431589 ACC : 0.7325085324232082\n",
      "VALID AUC : 0.8071094686792408 ACC : 0.7353233830845771\n",
      "\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.44246774911880493\n",
      "Training steps: 50 Loss: 0.45446598529815674\n",
      "TRAIN AUC : 0.8049541118800408 ACC : 0.7331484641638225\n",
      "VALID AUC : 0.8073177480830857 ACC : 0.7298507462686568\n",
      "\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.47156745195388794\n",
      "Training steps: 50 Loss: 0.4704066514968872\n",
      "TRAIN AUC : 0.8047801682366874 ACC : 0.7320819112627986\n",
      "VALID AUC : 0.8066046390765884 ACC : 0.735820895522388\n",
      "\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.4609581232070923\n",
      "Training steps: 50 Loss: 0.4421839118003845\n",
      "TRAIN AUC : 0.8059149043629041 ACC : 0.7327218430034129\n",
      "VALID AUC : 0.8077015772701711 ACC : 0.7353233830845771\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 21\n",
      "Training steps: 0 Loss: 0.4521099328994751\n",
      "Training steps: 50 Loss: 0.4373937249183655\n",
      "TRAIN AUC : 0.8056706721718431 ACC : 0.7337883959044369\n",
      "VALID AUC : 0.8072314609014928 ACC : 0.7343283582089553\n",
      "\n",
      "Start Training: Epoch 22\n",
      "Training steps: 0 Loss: 0.4542972445487976\n",
      "Training steps: 50 Loss: 0.4420989751815796\n",
      "TRAIN AUC : 0.8052795724705922 ACC : 0.7340017064846417\n",
      "VALID AUC : 0.8061265882544302 ACC : 0.7263681592039801\n",
      "\n",
      "Start Training: Epoch 23\n",
      "Training steps: 0 Loss: 0.4598335921764374\n",
      "Training steps: 50 Loss: 0.47879523038864136\n",
      "TRAIN AUC : 0.8056032097525132 ACC : 0.7348549488054608\n",
      "VALID AUC : 0.807829520332533 ACC : 0.7353233830845771\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 24\n",
      "Training steps: 0 Loss: 0.4892809987068176\n",
      "Training steps: 50 Loss: 0.46287745237350464\n",
      "TRAIN AUC : 0.8064154937474188 ACC : 0.7335750853242321\n",
      "VALID AUC : 0.8076966182367462 ACC : 0.7328358208955223\n",
      "\n",
      "Start Training: Epoch 25\n",
      "Training steps: 0 Loss: 0.4543616771697998\n",
      "Training steps: 50 Loss: 0.4468190371990204\n",
      "TRAIN AUC : 0.8057650283934736 ACC : 0.7325085324232082\n",
      "VALID AUC : 0.8068476317144073 ACC : 0.7328358208955223\n",
      "\n",
      "Start Training: Epoch 26\n",
      "Training steps: 0 Loss: 0.4598259925842285\n",
      "Training steps: 50 Loss: 0.45659512281417847\n",
      "TRAIN AUC : 0.8069336780602184 ACC : 0.7297354948805461\n",
      "VALID AUC : 0.8088600074782225 ACC : 0.7308457711442786\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 27\n",
      "Training steps: 0 Loss: 0.44575536251068115\n",
      "Training steps: 50 Loss: 0.4699568748474121\n",
      "TRAIN AUC : 0.8074338116175755 ACC : 0.7372013651877133\n",
      "VALID AUC : 0.8076301671888528 ACC : 0.7368159203980099\n",
      "\n",
      "Start Training: Epoch 28\n",
      "Training steps: 0 Loss: 0.45655813813209534\n",
      "Training steps: 50 Loss: 0.4482946991920471\n",
      "TRAIN AUC : 0.8064515040928719 ACC : 0.7329351535836177\n",
      "VALID AUC : 0.8073227071165106 ACC : 0.7323383084577114\n",
      "\n",
      "Start Training: Epoch 29\n",
      "Training steps: 0 Loss: 0.4478555917739868\n",
      "Training steps: 50 Loss: 0.4616834223270416\n",
      "TRAIN AUC : 0.8055618206466 ACC : 0.7322952218430034\n",
      "VALID AUC : 0.8070033453639485 ACC : 0.7353233830845771\n",
      "\n",
      "Start Training: Epoch 30\n",
      "Training steps: 0 Loss: 0.4761035442352295\n",
      "Training steps: 50 Loss: 0.4754982888698578\n",
      "TRAIN AUC : 0.8071444525379088 ACC : 0.7325085324232082\n",
      "VALID AUC : 0.8067236558787854 ACC : 0.7388059701492538\n",
      "\n",
      "Start Training: Epoch 31\n",
      "Training steps: 0 Loss: 0.4233754277229309\n",
      "Training steps: 50 Loss: 0.4720348119735718\n",
      "TRAIN AUC : 0.806820997586851 ACC : 0.7337883959044369\n",
      "VALID AUC : 0.8065024829880358 ACC : 0.7298507462686568\n",
      "\n",
      "Start Training: Epoch 32\n",
      "Training steps: 0 Loss: 0.4479621946811676\n",
      "Training steps: 50 Loss: 0.43915101885795593\n",
      "TRAIN AUC : 0.8067487033996501 ACC : 0.7333617747440273\n",
      "VALID AUC : 0.8068734186882167 ACC : 0.7343283582089553\n",
      "\n",
      "Start Training: Epoch 33\n",
      "Training steps: 0 Loss: 0.44770923256874084\n",
      "Training steps: 50 Loss: 0.45388537645339966\n",
      "TRAIN AUC : 0.8057616552725072 ACC : 0.7310153583617748\n",
      "VALID AUC : 0.8067058033584559 ACC : 0.7348258706467662\n",
      "\n",
      "Start Training: Epoch 34\n",
      "Training steps: 0 Loss: 0.47533196210861206\n",
      "Training steps: 50 Loss: 0.4486796259880066\n",
      "TRAIN AUC : 0.8062056309240436 ACC : 0.7314419795221843\n",
      "VALID AUC : 0.8062912281641361 ACC : 0.7293532338308458\n",
      "\n",
      "Start Training: Epoch 35\n",
      "Training steps: 0 Loss: 0.4807838201522827\n",
      "Training steps: 50 Loss: 0.45602020621299744\n",
      "TRAIN AUC : 0.8070034196153366 ACC : 0.7327218430034129\n",
      "VALID AUC : 0.8070321077578129 ACC : 0.7343283582089553\n",
      "\n",
      "Start Training: Epoch 36\n",
      "Training steps: 0 Loss: 0.4610331654548645\n",
      "Training steps: 50 Loss: 0.45538005232810974\n",
      "TRAIN AUC : 0.806777238179718 ACC : 0.7303754266211604\n",
      "VALID AUC : 0.8070172306575382 ACC : 0.7333333333333333\n",
      "\n",
      "Start Training: Epoch 37\n",
      "Training steps: 0 Loss: 0.46770989894866943\n",
      "Training steps: 50 Loss: 0.476613849401474\n",
      "TRAIN AUC : 0.8062791102618545 ACC : 0.7322952218430034\n",
      "VALID AUC : 0.8064013187061683 ACC : 0.7303482587064677\n",
      "\n",
      "Epoch    37: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Start Training: Epoch 38\n",
      "Training steps: 0 Loss: 0.4464828073978424\n",
      "Training steps: 50 Loss: 0.4954434931278229\n",
      "TRAIN AUC : 0.8086215148595095 ACC : 0.7327218430034129\n",
      "VALID AUC : 0.8074080024914183 ACC : 0.7368159203980099\n",
      "\n",
      "Start Training: Epoch 39\n",
      "Training steps: 0 Loss: 0.44544458389282227\n",
      "Training steps: 50 Loss: 0.45281922817230225\n",
      "TRAIN AUC : 0.8078933765579033 ACC : 0.735494880546075\n",
      "VALID AUC : 0.807053927504882 ACC : 0.7318407960199005\n",
      "\n",
      "Start Training: Epoch 40\n",
      "Training steps: 0 Loss: 0.44369029998779297\n",
      "Training steps: 50 Loss: 0.48195961117744446\n",
      "TRAIN AUC : 0.8082641463679237 ACC : 0.7350682593856656\n",
      "VALID AUC : 0.8070906243522262 ACC : 0.7328358208955223\n",
      "\n",
      "Start Training: Epoch 41\n",
      "Training steps: 0 Loss: 0.45731407403945923\n",
      "Training steps: 50 Loss: 0.4606083631515503\n",
      "TRAIN AUC : 0.808601002637416 ACC : 0.7359215017064846\n",
      "VALID AUC : 0.8070182224642232 ACC : 0.736318407960199\n",
      "\n",
      "EarlyStopping counter: 15 out of 15\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 11947<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80342f7d722e4ea9a4c9e9dc6fff971e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/opt/ml/wandb/run-20210603_051023-28o82f6e/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/opt/ml/wandb/run-20210603_051023-28o82f6e/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>40</td></tr><tr><td>train_loss</td><td>0.45355</td></tr><tr><td>train_auc</td><td>0.8086</td></tr><tr><td>train_acc</td><td>0.73592</td></tr><tr><td>valid_auc</td><td>0.80702</td></tr><tr><td>valid_acc</td><td>0.73632</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>_runtime</td><td>239</td></tr><tr><td>_timestamp</td><td>1622697263</td></tr><tr><td>_step</td><td>40</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▁▃▃▃▄▅▇▇▇▇▇█████████████████████████████</td></tr><tr><td>train_acc</td><td>▁▄▄▄▅▆▇▇████████████████████████████████</td></tr><tr><td>valid_auc</td><td>▁▁▂▃▄▆▇█████████████████████████████████</td></tr><tr><td>valid_acc</td><td>▁▁▁▃▄▅▆▇█▇██▇▇▇▇█▇███▇█▇▇▇█▇██▇██▇█▇▇█▇█</td></tr><tr><td>lr</td><td>█████████████████████████████████████▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">fresh-totem-78</strong>: <a href=\"https://wandb.ai/cha-no/DKT/runs/28o82f6e\" target=\"_blank\">https://wandb.ai/cha-no/DKT/runs/28o82f6e</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "        \n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    \n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, scheduler, args)\n",
    "    \n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    lr = get_lr(optimizer)\n",
    "    ### TODO: model save or early stopping\n",
    "    # wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "    #             \"valid_auc\":auc, \"valid_acc\":acc, \"lr\":lr})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        # delete_model(args.model_dir)\n",
    "        # save_checkpoint({\n",
    "        #     'epoch': epoch + 1,\n",
    "        #     'state_dict': model_to_save.state_dict(),\n",
    "        #     },\n",
    "        #     args.model_dir, f'model_{epoch + 1}.pt',\n",
    "        # )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)\n",
    "\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(args.test_file_name)\n",
    "test_data = preprocess.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):    \n",
    "    model_path = args.model_dir\n",
    "    print(\"Loading Model from:\", args.model_dir)\n",
    "    load_state = torch.load(model_path)\n",
    "\n",
    "    if args.model == 'customlstm':\n",
    "        model = CustomLSTM(args)\n",
    "    elif args.model == 'customlstmattn':\n",
    "        model = CustomLSTMATTN(args)\n",
    "    elif args.model == 'custombert':\n",
    "        model = CustomBert(args)\n",
    "    elif args.model == 'customxlm':\n",
    "        model = CustomXlmRoberta(args)\n",
    "    elif args.model == 'customsaint':\n",
    "        model = CustomSaint(args)\n",
    "    elif args.model == 'customlastquery':\n",
    "        model = CustomLastQuery(args)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "    \n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model from: /opt/ml/models/customlastquery5/model_43.pt\nLoading Model from: /opt/ml/models/customlastquery5/model_43.pt ...Finished.\n\n"
     ]
    }
   ],
   "source": [
    "# args.model_dir = '/opt/ml/models'\n",
    "# args.model = 'customlastquery'\n",
    "# args.model_name = 'customlastquery5/model_43.pt'\n",
    "# args.model_dir = os.path.join(args.model_dir, args.model_name)\n",
    "model = load_model(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args, test_data):\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "    \n",
    "    output_path = os.path.dirname(args.model_name)\n",
    "    os.makedirs(os.path.join(args.output_dir, output_path), exist_ok=True)\n",
    "    write_path = os.path.join(args.output_dir, output_path, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    print(write_path)\n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/output/customlastquery5/output.csv\nwriting prediction : /opt/ml/output/customlastquery5/output.csv\n"
     ]
    }
   ],
   "source": [
    "inference(args, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}