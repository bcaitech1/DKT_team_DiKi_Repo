{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93636e67",
   "metadata": {},
   "source": [
    "# 연속형변수 추가한 baseline코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaa65a72-1bf1-44f7-9da7-6a21914459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "import pdb\n",
    "import wandb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel\n",
    "\n",
    "from dkt.dataloader import Preprocess\n",
    "from dkt import trainer\n",
    "from dkt.utils import setSeeds, increment_path\n",
    "from dkt.optimizer import get_optimizer\n",
    "from dkt.scheduler import get_scheduler\n",
    "from dkt.trainer import compute_loss, update_params, get_lr, save_checkpoint\n",
    "from dkt.metric import get_metric\n",
    "from dkt.criterion import get_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa52652-164e-47e6-bcf3-e1f11c3e85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = {\n",
    "    'seed' : 42,\n",
    "    'device' : 'cuda',\n",
    "    'data_dir' : '/opt/ml/input/data/train_dataset',\n",
    "    'asset_dir' : '/opt/ml/code/T_1170_LeeHakYoung/asset/',\n",
    "    'file_name' : 'train_data.csv',\n",
    "    'model_dir' : '/opt/ml/code/T_1170_LeeHakYoung/models/',\n",
    "    'model_name' : 'model.pt',\n",
    "    'output_dir' : '/opt/ml/code/T_1170_LeeHakYoung/output/',\n",
    "    'test_file_name' : 'test_data.csv',\n",
    "    'max_seq_len' : 1024,\n",
    "    'num_workers' : 1,\n",
    "    'hidden_dim' : 512,\n",
    "    'n_layers' : 2,\n",
    "    'n_heads' : 2,\n",
    "    'drop_out' : 0.2,\n",
    "    'n_epochs' : 200,\n",
    "    'batch_size' : 64,\n",
    "    'lr' : 0.0001,\n",
    "    'clip_grad' : 10,\n",
    "    'patience' : 15,\n",
    "    'log_steps' : 50,\n",
    "    'model' : 'lstm',\n",
    "    'optimizer' : 'adamW',\n",
    "    'scheduler' : 'plateau'\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**namespace)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "args.device = device\n",
    "setSeeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb675f0e-7713-4a65-8882-5bf887908de9",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "\n",
    " preprocess\n",
    "\n",
    "- __feature_engineering함수에서 연속형 변수 처리해줬고 그에 맞게 다른 함수도 처리했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9098a1c4-d70a-4a19-82de-fe7bcdaa9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.7, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if str(x) in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        def convert_time(s):\n",
    "            timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "            return int(timestamp)\n",
    "\n",
    "        df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        # TODO\n",
    "        def percentile(s):\n",
    "            return np.sum(s) / len(s)\n",
    "        \n",
    "        # 큰 카테고리\n",
    "        df['big_features'] = df['testId'].apply(lambda x : x[2]).astype(int)\n",
    "\n",
    "        # 큰 카테고리별 정답률\n",
    "        stu_groupby = df.groupby('big_features').agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # tag별 정답률\n",
    "        stu_tag_groupby = df.groupby(['big_features', 'KnowledgeTag']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "\n",
    "        # 시험지별 정답률\n",
    "        stu_test_groupby = df.groupby(['big_features', 'testId']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'answerCode' : 'answer_rate'})\n",
    "                                                                    \n",
    "        # 문항별 정답률\n",
    "        stu_assessment_groupby = df.groupby(['big_features', 'assessmentItemID']).agg({\n",
    "            'assessmentItemID': 'count',\n",
    "            'answerCode': percentile\n",
    "        }).rename(columns = {'assessmentItemID' : 'assessment_count', 'answerCode' : 'answer_rate'})\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "\n",
    "        # 정답 - 큰 카테고리별 정답률 \n",
    "        '''ex)\n",
    "        맞은 문제의 큰 카테고리별 정답률이 0.7 이면 1 - 0.7 = 0.3이 됨)\n",
    "        틀린 문제의 큰 카테고리별 정답률이 0.7 이면 0 - 0.7 = -0.7이 됨)\n",
    "        '''\n",
    "        temp = pd.merge(df, stu_groupby.reset_index()[['big_features', 'answer_rate']], on = ['big_features'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['answer_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 태그별 정답률\n",
    "        temp = pd.merge(df, stu_tag_groupby.reset_index()[['answer_rate', 'KnowledgeTag']], on = ['KnowledgeTag'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['tag_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 시험별 정답률\n",
    "        temp = pd.merge(df, stu_test_groupby.reset_index()[['answer_rate', 'testId']], on = ['testId'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['test_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        # 정답 - 문항별 정답률\n",
    "        temp = pd.merge(df, stu_assessment_groupby.reset_index()[['answer_rate', 'assessmentItemID']], on = ['assessmentItemID'])\n",
    "        temp = temp.sort_values(by=['userID','Timestamp'], axis=0).reset_index()\n",
    "        df['assess_delta'] = temp['answerCode'] - temp['answer_rate']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        self.args.n_big_features = 9\n",
    "\n",
    "\n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag', 'big_features', 'answer_delta', 'tag_delta', 'test_delta', 'assess_delta']\n",
    "\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values,\n",
    "                    r['big_features'].values,\n",
    "                    r['answer_delta'].values,\n",
    "                    r['tag_delta'].values,\n",
    "                    r['test_delta'].values,\n",
    "                    r['assess_delta'].values,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47ab77a3-fff8-4265-92ff-acbb5819fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(args.file_name)\n",
    "train_data = preprocess.get_train_data()\n",
    "\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd256ef1",
   "metadata": {},
   "source": [
    "## DKTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b281f95-809a-48fe-b797-a2c4eb4607e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct, big_features, answer_delta, tag_delta, test_delta, assess_delta = row\n",
    "\n",
    "        # category변수와 continuout변수를 나눠줌        \n",
    "        cate_cols = [test, question, tag, correct, big_features]\n",
    "        cont_cols = [answer_delta, tag_delta, test_delta, assess_delta]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cont_cols):\n",
    "                cont_cols[i] = col[-self.args.max_seq_len:]\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cont_cols):\n",
    "            cont_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols, cont_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    # cate변수에서 했던 처리를 cont에서도 똑같이 해줌\n",
    "    cate_col_n = len(batch[0][0])\n",
    "    cont_col_n = len(batch[0][1])\n",
    "\n",
    "    cate_col_list = [[] for _ in range(cate_col_n)]\n",
    "    cont_col_list = [[] for _ in range(cont_col_n)]\n",
    "\n",
    "    max_seq_len = len(batch[0][0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row[0]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cate_col_list[i].append(pre_padded)\n",
    "        for i, col in enumerate(row[1]):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            cont_col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(cate_col_list):\n",
    "        cate_col_list[i] =torch.stack(cate_col_list[i])\n",
    "    \n",
    "    for i, _ in enumerate(cont_col_list):\n",
    "        cont_col_list[i] =torch.stack(cont_col_list[i])\n",
    "\n",
    "    return tuple(cate_col_list), tuple(cont_col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ba5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33151d8e",
   "metadata": {},
   "source": [
    "### process_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9607527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, args):\n",
    "\n",
    "    (test, question, tag, correct, big_features, mask), cont_features = batch    \n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "    big_features = big_features.type(torch.FloatTensor)\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "    big_features = (big_features * mask).to(torch.int64)\n",
    "\n",
    "    # interaction과 동일하게 rolling을 해서 이전 정보를 사용할 수 있도록 함\n",
    "    for cont_feature in cont_features:\n",
    "        cont_feature = cont_feature.type(torch.FloatTensor)\n",
    "        cont_feature = cont_feature.roll(shifts=1, dims=1)\n",
    "        cont_feature[:, 0] = 0\n",
    "        cont_feature = (cont_feature * mask).unsqueeze(-1)\n",
    "        temp.append(cont_feature)\n",
    "    \n",
    "    # device memory로 이동\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "    interaction = interaction.to(args.device)\n",
    "    big_features = big_features.to(args.device)\n",
    "\n",
    "    # 연속형 변수들을 concat해줌\n",
    "    cont_features = torch.cat(temp, dim=-1).to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, big_features), cont_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d4bad",
   "metadata": {},
   "source": [
    "## model\n",
    "\n",
    "- lstm, bert 모델을 사용했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f4ea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # 큰 카테고리 embedding 추가\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "        \n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*5, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(4, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "\n",
    "        # cate변수와 cont변수를 concat해서 lstm input으로 넣어줌        \n",
    "        X = torch.cat([cate_embed, cont_embed], 2)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd4170c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "        # 큰 카테고리 embedding 추가\n",
    "        self.embedding_big = nn.Embedding(self.args.n_big_features + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Sequential(\n",
    "            nn.Linear((self.hidden_dim//3)*5, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # cont features\n",
    "        self.cont_embed = nn.Sequential(\n",
    "            nn.Linear(4, self.hidden_dim//2),\n",
    "            nn.LayerNorm(self.hidden_dim//2)\n",
    "        )\n",
    "\n",
    "        # Bert config\n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=self.args.n_layers,\n",
    "            num_attention_heads=self.args.n_heads,\n",
    "            max_position_embeddings=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        (test, question, tag, _, mask, interaction, big_features), cont_features = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        embed_big = self.embedding_big(big_features)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_big,\n",
    "                           embed_tag,\n",
    "                           ], 2)\n",
    "\n",
    "        cate_embed = self.comb_proj(embed)\n",
    "        cont_embed = self.cont_embed(cont_features)\n",
    "\n",
    "        # cate변수와 cont변수를 concat해서 bert의 input에 넣어줌        \n",
    "        X = torch.cat([cate_embed, cont_embed], 2)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers[0]\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36854e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    # loss = loss[:,-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921abc09",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14ccd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "\n",
    "        # cont변수가 추가되었으므로 처리가 필요함 어쨋든 ground truth 인 정답\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "\n",
    "        # 마찬가지로 정답\n",
    "        targets = input[0][3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "074e295d",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.model = 'custombert'\n",
    "\n",
    "if args.model == 'customlstm':\n",
    "    model = CustomLSTM(args)\n",
    "elif args.model == 'custombert':\n",
    "    model = CustomBert(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d723e7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">rose-paper-138</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/discone1008/dkt\" target=\"_blank\">https://wandb.ai/discone1008/dkt</a><br/>\n                Run page: <a href=\"https://wandb.ai/discone1008/dkt/runs/2kc3kusx\" target=\"_blank\">https://wandb.ai/discone1008/dkt/runs/2kc3kusx</a><br/>\n                Run data is saved locally in <code>/opt/ml/code/T_1170_LeeHakYoung/wandb/run-20210601_061354-2kc3kusx</code><br/><br/>\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# 저는 추가한 feature를 wandb에 기록해주었습니다\n",
    "args.add_features = [\"bigfeature\",\"answer_delta\",\"tag_delta\",\"test_delta\",\"assess_delta\"]\n",
    "name = f'fe4_maxseq{args.max_seq_len}_hiddendim{args.hidden_dim}_custombert'\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project='dkt', config=vars(args))\n",
    "wandb.run.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8dd7c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.9115465879440308\n",
      "Training steps: 50 Loss: 0.17245393991470337\n",
      "TRAIN AUC : 0.6654689960042192 ACC : 0.609641638225256\n",
      "VALID AUC : 0.7303466066821984 ACC : 0.6532338308457711\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.18259629607200623\n",
      "Training steps: 50 Loss: 0.15632671117782593\n",
      "TRAIN AUC : 0.7225846858484812 ACC : 0.6523037542662116\n",
      "VALID AUC : 0.7460409556652493 ACC : 0.6646766169154229\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.1870344877243042\n",
      "Training steps: 50 Loss: 0.18626393377780914\n",
      "TRAIN AUC : 0.7384212152899015 ACC : 0.6623293515358362\n",
      "VALID AUC : 0.7537165476002741 ACC : 0.681592039800995\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.15378251671791077\n",
      "Training steps: 50 Loss: 0.17252960801124573\n",
      "TRAIN AUC : 0.7468955891429265 ACC : 0.674061433447099\n",
      "VALID AUC : 0.752391493869147 ACC : 0.6736318407960199\n",
      "\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.17742113769054413\n",
      "Training steps: 50 Loss: 0.1708213984966278\n",
      "TRAIN AUC : 0.749873963790914 ACC : 0.6802474402730375\n",
      "VALID AUC : 0.7586864908986859 ACC : 0.6850746268656717\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.17054440081119537\n",
      "Training steps: 50 Loss: 0.15169817209243774\n",
      "TRAIN AUC : 0.7544358819845255 ACC : 0.6819539249146758\n",
      "VALID AUC : 0.7603914065901587 ACC : 0.6865671641791045\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.15187591314315796\n",
      "Training steps: 50 Loss: 0.1580820083618164\n",
      "TRAIN AUC : 0.7586288536767474 ACC : 0.6892064846416383\n",
      "VALID AUC : 0.7657392282355462 ACC : 0.6915422885572139\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.1393771469593048\n",
      "Training steps: 50 Loss: 0.16494086384773254\n",
      "TRAIN AUC : 0.7601859592472288 ACC : 0.6917662116040956\n",
      "VALID AUC : 0.7672462784933664 ACC : 0.6900497512437811\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.1790040284395218\n",
      "Training steps: 50 Loss: 0.14750723540782928\n",
      "TRAIN AUC : 0.7586572061259523 ACC : 0.6943259385665529\n",
      "VALID AUC : 0.7685321558604369 ACC : 0.699502487562189\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.1994456946849823\n",
      "Training steps: 50 Loss: 0.1307159960269928\n",
      "TRAIN AUC : 0.7637770567605093 ACC : 0.7039249146757679\n",
      "VALID AUC : 0.7732521638742349 ACC : 0.7054726368159204\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.12238095700740814\n",
      "Training steps: 50 Loss: 0.1200156956911087\n",
      "TRAIN AUC : 0.7649806227875289 ACC : 0.6983788395904437\n",
      "VALID AUC : 0.7776309903884016 ACC : 0.7109452736318408\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.1454145759344101\n",
      "Training steps: 50 Loss: 0.1315355896949768\n",
      "TRAIN AUC : 0.7715276682526413 ACC : 0.70669795221843\n",
      "VALID AUC : 0.78292922169954 ACC : 0.7159203980099502\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.1175413578748703\n",
      "Training steps: 50 Loss: 0.14608702063560486\n",
      "TRAIN AUC : 0.7793854902922491 ACC : 0.7128839590443686\n",
      "VALID AUC : 0.7840450042201375 ACC : 0.718407960199005\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.1211516484618187\n",
      "Training steps: 50 Loss: 0.13236144185066223\n",
      "TRAIN AUC : 0.7819138723702191 ACC : 0.7186433447098977\n",
      "VALID AUC : 0.7880806656213024 ACC : 0.7213930348258707\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.16128307580947876\n",
      "Training steps: 50 Loss: 0.13197539746761322\n",
      "TRAIN AUC : 0.7842453371160909 ACC : 0.7145904436860068\n",
      "VALID AUC : 0.784140217661895 ACC : 0.7129353233830846\n",
      "\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.11106127500534058\n",
      "Training steps: 50 Loss: 0.11128886789083481\n",
      "TRAIN AUC : 0.7864519052207708 ACC : 0.7186433447098977\n",
      "VALID AUC : 0.7814434952854469 ACC : 0.7074626865671642\n",
      "\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.10186293721199036\n",
      "Training steps: 50 Loss: 0.1196330189704895\n",
      "TRAIN AUC : 0.7807078448765482 ACC : 0.7197098976109215\n",
      "VALID AUC : 0.7803485407052341 ACC : 0.7014925373134329\n",
      "\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.099861741065979\n",
      "Training steps: 50 Loss: 0.11980299651622772\n",
      "TRAIN AUC : 0.7809382199220171 ACC : 0.7090443686006825\n",
      "VALID AUC : 0.7797162639435623 ACC : 0.708457711442786\n",
      "\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.08952456712722778\n",
      "Training steps: 50 Loss: 0.0820334181189537\n",
      "TRAIN AUC : 0.7792141904464097 ACC : 0.7096843003412969\n",
      "VALID AUC : 0.7790066262604622 ACC : 0.7074626865671642\n",
      "\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.08049655705690384\n",
      "Training steps: 50 Loss: 0.0712820440530777\n",
      "TRAIN AUC : 0.7839667355573443 ACC : 0.7169368600682594\n",
      "VALID AUC : 0.7778997700000299 ACC : 0.708457711442786\n",
      "\n",
      "Start Training: Epoch 21\n",
      "Training steps: 0 Loss: 0.10407350957393646\n",
      "Training steps: 50 Loss: 0.09188474714756012\n",
      "TRAIN AUC : 0.7795824076243474 ACC : 0.7109641638225256\n",
      "VALID AUC : 0.7793864882208079 ACC : 0.7124378109452736\n",
      "\n",
      "Start Training: Epoch 22\n",
      "Training steps: 0 Loss: 0.08919006586074829\n",
      "Training steps: 50 Loss: 0.0796685665845871\n",
      "TRAIN AUC : 0.7808861644606152 ACC : 0.715443686006826\n",
      "VALID AUC : 0.7794797180491957 ACC : 0.7174129353233831\n",
      "\n",
      "Start Training: Epoch 23\n",
      "Training steps: 0 Loss: 0.0812099352478981\n",
      "Training steps: 50 Loss: 0.09312871098518372\n",
      "TRAIN AUC : 0.7844423456136209 ACC : 0.7169368600682594\n",
      "VALID AUC : 0.776234526575956 ACC : 0.7119402985074627\n",
      "\n",
      "Start Training: Epoch 24\n",
      "Training steps: 0 Loss: 0.09731127321720123\n",
      "Training steps: 50 Loss: 0.06643830239772797\n",
      "TRAIN AUC : 0.7886185428664416 ACC : 0.7184300341296929\n",
      "VALID AUC : 0.7804080491063325 ACC : 0.7104477611940299\n",
      "\n",
      "Start Training: Epoch 25\n",
      "Training steps: 0 Loss: 0.060800399631261826\n",
      "Training steps: 50 Loss: 0.07672439515590668\n",
      "TRAIN AUC : 0.7887159987127441 ACC : 0.7216296928327645\n",
      "VALID AUC : 0.782685237255036 ACC : 0.7213930348258707\n",
      "\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Start Training: Epoch 26\n",
      "Training steps: 0 Loss: 0.06502934545278549\n",
      "Training steps: 50 Loss: 0.06717471033334732\n",
      "TRAIN AUC : 0.7847427357105023 ACC : 0.7109641638225256\n",
      "VALID AUC : 0.7802890323041354 ACC : 0.7109452736318408\n",
      "\n",
      "Start Training: Epoch 27\n",
      "Training steps: 0 Loss: 0.056215859949588776\n",
      "Training steps: 50 Loss: 0.05734030902385712\n",
      "TRAIN AUC : 0.7904237095761081 ACC : 0.7207764505119454\n",
      "VALID AUC : 0.7791484546164139 ACC : 0.7124378109452736\n",
      "\n",
      "Start Training: Epoch 28\n",
      "Training steps: 0 Loss: 0.061472974717617035\n",
      "Training steps: 50 Loss: 0.056853409856557846\n",
      "TRAIN AUC : 0.7836482035395891 ACC : 0.7103242320819113\n",
      "VALID AUC : 0.7772853457586875 ACC : 0.7044776119402985\n",
      "\n",
      "Start Training: Epoch 29\n",
      "Training steps: 0 Loss: 0.07454247772693634\n",
      "Training steps: 50 Loss: 0.060717519372701645\n",
      "TRAIN AUC : 0.7870556027083425 ACC : 0.7184300341296929\n",
      "VALID AUC : 0.7785136983380296 ACC : 0.7074626865671642\n",
      "\n",
      "EarlyStopping counter: 15 out of 15\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 28796<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60f2d49fbf0e4b48b77ae0d08f2b8933"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/opt/ml/code/T_1170_LeeHakYoung/wandb/run-20210601_061354-2kc3kusx/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/opt/ml/code/T_1170_LeeHakYoung/wandb/run-20210601_061354-2kc3kusx/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>28</td></tr><tr><td>train_loss</td><td>0.06137</td></tr><tr><td>train_auc</td><td>0.78706</td></tr><tr><td>train_acc</td><td>0.71843</td></tr><tr><td>valid_auc</td><td>0.77851</td></tr><tr><td>valid_acc</td><td>0.70746</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>_runtime</td><td>1779</td></tr><tr><td>_timestamp</td><td>1622529813</td></tr><tr><td>_step</td><td>28</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▁▄▅▆▆▆▆▆▆▇▇▇▇███▇▇▇█▇▇███████</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▆▆▆▆▇▇▇▇████▇▇█▇████▇█▇█</td></tr><tr><td>valid_auc</td><td>▁▃▄▄▄▅▅▅▆▆▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>valid_acc</td><td>▁▂▄▃▄▄▅▅▆▆▇▇██▇▇▆▇▇▇▇█▇▇█▇▇▆▇</td></tr><tr><td>lr</td><td>█████████████████████████▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">rose-paper-138</strong>: <a href=\"https://wandb.ai/discone1008/dkt/runs/2kc3kusx\" target=\"_blank\">https://wandb.ai/discone1008/dkt/runs/2kc3kusx</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "args.model_dir = increment_path(os.path.join(args.model_dir, args.model))\n",
    "os.makedirs(args.model_dir, exist_ok=True)    \n",
    "\n",
    "# only when using warmup scheduler\n",
    "args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "args.warmup_steps = args.total_steps // 10\n",
    "        \n",
    "optimizer = get_optimizer(model, args)\n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "best_auc = -1\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "    \n",
    "    ### TRAIN\n",
    "    train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "    \n",
    "    ### VALID\n",
    "    auc, acc,_ , _ = validate(valid_loader, model, args)\n",
    "\n",
    "    lr = get_lr(optimizer)\n",
    "    ### TODO: model save or early stopping\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                \"valid_auc\":auc, \"valid_acc\":acc, \"lr\":lr})\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        # # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        \n",
    "        # 이함수는 제가 추가한 함수라서 주석처리해주시면 됩니다\n",
    "        # delete_model(args.model_dir)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model_to_save.state_dict(),\n",
    "            },\n",
    "            args.model_dir, f'model_{epoch + 1}.pt',\n",
    "        )\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= args.patience:\n",
    "            print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "            break\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler.step(best_auc)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1df4d7",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c869a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(args.test_file_name)\n",
    "test_data = preprocess.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdcfcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_loader = get_loaders(args, None, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b28ce964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args):    \n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    print(\"Loading Model from:\", model_path)\n",
    "    load_state = torch.load(model_path)\n",
    "\n",
    "    if args.model == 'custombert':\n",
    "        model = CustomBert(args)\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "    \n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d562aca",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Model from: /opt/ml/code/T_1170_LeeHakYoung/models/custombert2/model_14.pt\n",
      "Loading Model from: /opt/ml/code/T_1170_LeeHakYoung/models/custombert2/model_14.pt ...Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.model_name = 'model_14.pt'\n",
    "model = load_model(args)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb13c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args, test_data):\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "    \n",
    "    os.makedirs(os.path.join(args.output_dir, args.model), exist_ok=True)\n",
    "    write_path = os.path.join(args.output_dir, args.model, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    print(write_path)\n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48e74a00",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/code/T_1170_LeeHakYoung/output/custombert/output.csv\nwriting prediction : /opt/ml/code/T_1170_LeeHakYoung/output/custombert/output.csv\n"
     ]
    }
   ],
   "source": [
    "inference(args, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}