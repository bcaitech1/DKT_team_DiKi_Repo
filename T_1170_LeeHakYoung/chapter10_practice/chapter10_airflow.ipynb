{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow, Mlflow를 활용한 ML Cycle\n",
    "\n",
    "## MNIST 손글씨 분석을 위한 데이터 수집-모델 훈련-배포 과정 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관련 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: SQLAlchemy 1.3.24\n",
      "Uninstalling SQLAlchemy-1.3.24:\n",
      "  Successfully uninstalled SQLAlchemy-1.3.24\n",
      "Collecting sqlalchemy<1.4.0\n",
      "  Using cached SQLAlchemy-1.3.24-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: apache-airflow in /opt/conda/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: attrdict in /opt/conda/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied: mlflow in /opt/conda/lib/python3.7/site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy; python_version >= \"3.7\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.20.2)\n",
      "Requirement already satisfied: graphviz>=0.12 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.16)\n",
      "Requirement already satisfied: colorlog>=4.0.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (5.0.1)\n",
      "Requirement already satisfied: setproctitle<2,>=1.1.8 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.2.2)\n",
      "Requirement already satisfied: flask-appbuilder~=3.3 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (3.3.0)\n",
      "Requirement already satisfied: sqlalchemy-jsonfield~=1.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.0.0)\n",
      "Requirement already satisfied: flask-login<0.5,>=0.3 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (3.10.0.0)\n",
      "Requirement already satisfied: blinker in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.1.0)\n",
      "Requirement already satisfied: cattrs~=1.1; python_version > \"3.6\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.7.1)\n",
      "Requirement already satisfied: pyjwt<2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata~=1.7; python_version < \"3.9\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.7.0)\n",
      "Requirement already satisfied: pandas<2.0,>=0.17.1; python_version >= \"3.7\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.2.4)\n",
      "Requirement already satisfied: flask<2.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.1.2)\n",
      "Requirement already satisfied: flask-wtf<0.15,>=0.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.14.3)\n",
      "Requirement already satisfied: apache-airflow-providers-ftp in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.1.0)\n",
      "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (5.7.0)\n",
      "Requirement already satisfied: lazy-object-proxy in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.6.0)\n",
      "Requirement already satisfied: unicodecsv>=0.14.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.3 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.8.1)\n",
      "Requirement already satisfied: attrs<21.0,>=20.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (20.3.0)\n",
      "Requirement already satisfied: importlib-resources~=1.4 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.5.0)\n",
      "Requirement already satisfied: apache-airflow-providers-sqlite in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.0.2)\n",
      "Requirement already satisfied: iso8601>=0.1.12 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.1.14)\n",
      "Requirement already satisfied: python3-openid~=3.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (3.2.0)\n",
      "Requirement already satisfied: croniter<1.1,>=0.3.17 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.0.13)\n",
      "Requirement already satisfied: alembic<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.4.1)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.3.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1,~=1.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.18.1)\n",
      "Requirement already satisfied: lockfile>=0.12.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.12.2)\n",
      "Requirement already satisfied: gunicorn>=19.5.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (20.1.0)\n",
      "Requirement already satisfied: jsonschema~=3.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (3.2.0)\n",
      "Requirement already satisfied: rich>=9.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (5.3.1)\n",
      "Requirement already satisfied: tenacity~=6.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (6.2.0)\n",
      "Requirement already satisfied: python-nvd3~=0.15.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.15.0)\n",
      "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.8.9)\n",
      "Requirement already satisfied: argcomplete~=1.10 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.12.3)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.5.1)\n",
      "Requirement already satisfied: apache-airflow-providers-imap in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.0.1)\n",
      "Requirement already satisfied: swagger-ui-bundle>=0.0.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.0.8)\n",
      "Requirement already satisfied: dill<0.4,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.3.3)\n",
      "Requirement already satisfied: openapi-spec-validator>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (0.3.1)\n",
      "Requirement already satisfied: pendulum~=2.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.1.2)\n",
      "Requirement already satisfied: python-slugify<5.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (4.0.1)\n",
      "Requirement already satisfied: cached-property~=1.5; python_version <= \"3.7\" in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.5.2)\n",
      "Requirement already satisfied: markdown<4.0,>=2.5.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (3.3.4)\n",
      "Requirement already satisfied: flask-caching<2.0.0,>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.10.1)\n",
      "Requirement already satisfied: jinja2<2.12.0,>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.11.3)\n",
      "Requirement already satisfied: pygments<3.0,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.6.1)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.1.0)\n",
      "Requirement already satisfied: cryptography>=0.9.3 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.9.2)\n",
      "Requirement already satisfied: clickclick>=1.2 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (20.10.2)\n",
      "Requirement already satisfied: markupsafe<2.0,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (1.1.1)\n",
      "Requirement already satisfied: marshmallow-oneofschema>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-airflow) (2.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from attrdict) (1.15.0)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.4.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from mlflow) (2021.1)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.14.3)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.1.17)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.17.0)\n",
      "Requirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (5.0.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.6.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.18.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (7.1.2)\n",
      "Requirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.23.0)\n",
      "Requirement already satisfied: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (1.2.5)\n",
      "Requirement already satisfied: marshmallow-enum<2,>=1.5.1 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (1.5.1)\n",
      "Requirement already satisfied: Flask-JWT-Extended<4,>=3.18 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (3.25.1)\n",
      "Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (0.36.7)\n",
      "Requirement already satisfied: marshmallow-sqlalchemy<0.24.0,>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (0.23.1)\n",
      "Requirement already satisfied: marshmallow<4,>=3 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (3.12.1)\n",
      "Requirement already satisfied: apispec[yaml]<4,>=3.3 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (3.3.2)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (0.4.4)\n",
      "Requirement already satisfied: email-validator<2,>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (1.1.2)\n",
      "Requirement already satisfied: prison<1.0.0,>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (0.1.3)\n",
      "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (2.5.1)\n",
      "Requirement already satisfied: Flask-Babel<2,>=1 in /opt/conda/lib/python3.7/site-packages (from flask-appbuilder~=3.3->apache-airflow) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata~=1.7; python_version < \"3.9\"->apache-airflow) (3.4.1)\n",
      "Requirement already satisfied: WTForms in /opt/conda/lib/python3.7/site-packages (from flask-wtf<0.15,>=0.14.3->apache-airflow) (2.3.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from python3-openid~=3.2->apache-airflow) (0.7.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic<2.0,>=1.2->apache-airflow) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic<2.0,>=1.2->apache-airflow) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-daemon>=2.2.4->apache-airflow) (46.4.0.post20200518)\n",
      "Requirement already satisfied: docutils in /opt/conda/lib/python3.7/site-packages (from python-daemon>=2.2.4->apache-airflow) (0.17.1)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /opt/conda/lib/python3.7/site-packages (from httpx->apache-airflow) (1.5.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx->apache-airflow) (2020.6.20)\n",
      "Requirement already satisfied: httpcore<0.14.0,>=0.13.0 in /opt/conda/lib/python3.7/site-packages (from httpx->apache-airflow) (0.13.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.7/site-packages (from httpx->apache-airflow) (1.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema~=3.0->apache-airflow) (0.17.3)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich>=9.2.0->apache-airflow) (0.9.1)\n",
      "Requirement already satisfied: openapi-schema-validator in /opt/conda/lib/python3.7/site-packages (from openapi-spec-validator>=0.2.4->apache-airflow) (0.1.5)\n",
      "Requirement already satisfied: pytzdata>=2020.1 in /opt/conda/lib/python3.7/site-packages (from pendulum~=2.0->apache-airflow) (2020.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify<5.0,>=3.0.0->apache-airflow) (1.3)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=0.9.3->apache-airflow) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow) (1.0.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow) (0.10.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (1.25.8)\n",
      "Requirement already satisfied: dnspython>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from email-validator<2,>=1.0.5->flask-appbuilder~=3.3->apache-airflow) (2.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/lib/python3.7/site-packages (from Flask-Babel<2,>=1->flask-appbuilder~=3.3->apache-airflow) (2.9.1)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /opt/conda/lib/python3.7/site-packages (from httpcore<0.14.0,>=0.13.0->httpx->apache-airflow) (0.12.0)\n",
      "Requirement already satisfied: anyio==3.* in /opt/conda/lib/python3.7/site-packages (from httpcore<0.14.0,>=0.13.0->httpx->apache-airflow) (3.1.0)\n",
      "Requirement already satisfied: isodate in /opt/conda/lib/python3.7/site-packages (from openapi-schema-validator->openapi-spec-validator>=0.2.4->apache-airflow) (0.6.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=0.9.3->apache-airflow) (2.20)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (4.0.0)\n",
      "Installing collected packages: sqlalchemy\n",
      "Successfully installed sqlalchemy-1.3.24\n"
     ]
    }
   ],
   "source": [
    "#sqlalchemy의 버전이 1.4 이상인 경우 에러가 발생합니다.\n",
    "!pip uninstall sqlalchemy -y\n",
    "!pip install 'sqlalchemy < 1.4.0' apache-airflow attrdict mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DB: sqlite:////opt/ml/airflow/airflow.db\n",
      "[\u001b[34m2021-06-10 06:47:06,614\u001b[0m] {\u001b[34mdb.py:\u001b[0m695} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
      "Initialization done\n"
     ]
    }
   ],
   "source": [
    "!airflow db init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 제공되는 예제 DAG들을 가리기 위해\n",
    "# 터미널에 다음과 같이 이동하여 해당 파일에 load example 옵션을 비활성화합니다.\n",
    "# (반드시 해줘야 하는 것은 아닙니다.)\n",
    "\n",
    "# cd ~/airflow\n",
    "# vim airflow.cfg\n",
    "\n",
    "## === airflow.cfg === ##\n",
    "# load_examples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘/opt/ml/airflow/dags’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/airflow/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 멀티 프로세싱 관련된 에러를 피하기 위해\n",
    "# https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr\n",
    "!export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "admin already exist in the db\n"
     ]
    }
   ],
   "source": [
    "#GUI를 위한 유저생성\n",
    "\n",
    "!airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --password 1234 \\\n",
    "    --role Admin \\\n",
    "    --email spiderman@superhero.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 켜서 다음 명령어 입력\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# airflow scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "The following NEW packages will be installed:\n",
      "  lsof\n",
      "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
      "Need to get 248 kB of archives.\n",
      "After this operation, 451 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 lsof amd64 4.89+dfsg-0.1 [248 kB]\n",
      "Fetched 248 kB in 2s (144 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package lsof.\n",
      "(Reading database ... 17034 files and directories currently installed.)\n",
      "Preparing to unpack .../lsof_4.89+dfsg-0.1_amd64.deb ...\n",
      "Unpacking lsof (4.89+dfsg-0.1) ...\n",
      "Setting up lsof (4.89+dfsg-0.1) ...\n",
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
     ]
    }
   ],
   "source": [
    "# 이미 6006번 포트를 잡고 있을 경우를 대비해 6006번 포트를 사용하는 프로세스를 종료해줍니다.\n",
    "!apt-get install lsof\n",
    "!kill -9 `lsof -t -i:6006`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#터미널을 하나 더 켜서 다음 명령어 입력 (Airflow GUI 실행)\n",
    "# export TZ=Asia/Seoul\n",
    "# export AIRFLOW_HOME=~/airflow\n",
    "# airflow webserver -p 6006\n",
    "\n",
    "# 이제 다음 주소로 접근 가능합니다 (텐서보드 접속 포트를 확인해주세요!)\n",
    "# http://<SERVER_IP>:6009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 Bash Operator를 기반으로 한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting /opt/ml/airflow/dags/simple_dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/airflow/dags/simple_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_pipeline',\n",
    "    # default_args=default_args,\n",
    "    description='A simple pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DB: sqlite:////opt/ml/airflow/airflow.db\n",
      "[\u001b[34m2021-06-10 06:48:50,854\u001b[0m] {\u001b[34mdb.py:\u001b[0m711} INFO\u001b[0m - Dropping tables that exist\u001b[0m\n",
      "[\u001b[34m2021-06-10 06:48:51,000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m154} INFO\u001b[0m - Context impl \u001b[01mSQLiteImpl\u001b[22m.\u001b[0m\n",
      "[\u001b[34m2021-06-10 06:48:51,000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m161} INFO\u001b[0m - Will assume \u001b[01mnon-transactional\u001b[22m DDL.\u001b[0m\n",
      "[\u001b[34m2021-06-10 06:48:51,133\u001b[0m] {\u001b[34mdb.py:\u001b[0m695} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> e3a246e0dc1, current schema\n",
      "INFO  [alembic.runtime.migration] Running upgrade e3a246e0dc1 -> 1507a7289a2f, create is_encrypted\n",
      "\u001b[1;33m/opt/conda/lib/python3.7/site-packages/alembic/ddl/\u001b[0m\u001b[1;33msqlite.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m41\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Skipping unsupported ALTER for creation of implicit constraintPlease refer to the batch mode feature which allows for SQLite migrations using a copy-and-move strategy.\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Running upgrade 1507a7289a2f -> 13eb55f81627, maintain history for compatibility with earlier migrations\n",
      "INFO  [alembic.runtime.migration] Running upgrade 13eb55f81627 -> 338e90f54d61, More logging into task_instance\n",
      "INFO  [alembic.runtime.migration] Running upgrade 338e90f54d61 -> 52d714495f0, job_id indices\n",
      "INFO  [alembic.runtime.migration] Running upgrade 52d714495f0 -> 502898887f84, Adding extra to Log\n",
      "INFO  [alembic.runtime.migration] Running upgrade 502898887f84 -> 1b38cef5b76e, add dagrun\n",
      "INFO  [alembic.runtime.migration] Running upgrade 1b38cef5b76e -> 2e541a1dcfed, task_duration\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2e541a1dcfed -> 40e67319e3a9, dagrun_config\n",
      "INFO  [alembic.runtime.migration] Running upgrade 40e67319e3a9 -> 561833c1c74b, add password column to user\n",
      "INFO  [alembic.runtime.migration] Running upgrade 561833c1c74b -> 4446e08588, dagrun start end\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4446e08588 -> bbc73705a13e, Add notification_sent column to sla_miss\n",
      "INFO  [alembic.runtime.migration] Running upgrade bbc73705a13e -> bba5a7cfc896, Add a column to track the encryption state of the 'Extra' field in connection\n",
      "INFO  [alembic.runtime.migration] Running upgrade bba5a7cfc896 -> 1968acfc09e3, add is_encrypted column to variable table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 1968acfc09e3 -> 2e82aab8ef20, rename user table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2e82aab8ef20 -> 211e584da130, add TI state index\n",
      "INFO  [alembic.runtime.migration] Running upgrade 211e584da130 -> 64de9cddf6c9, add task fails journal table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 64de9cddf6c9 -> f2ca10b85618, add dag_stats table\n",
      "INFO  [alembic.runtime.migration] Running upgrade f2ca10b85618 -> 4addfa1236f1, Add fractional seconds to mysql tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4addfa1236f1 -> 8504051e801b, xcom dag task indices\n",
      "INFO  [alembic.runtime.migration] Running upgrade 8504051e801b -> 5e7d17757c7a, add pid field to TaskInstance\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5e7d17757c7a -> 127d2bf2dfa7, Add dag_id/state index on dag_run table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 127d2bf2dfa7 -> cc1e65623dc7, add max tries column to task instance\n",
      "ERROR [airflow.models.dagbag.DagBag] Failed to import: /opt/ml/airflow/dags/simple_dag.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/dagbag.py\", line 317, in _load_modules_from_file\n",
      "    loader.exec_module(new_module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/opt/ml/airflow/dags/simple_dag.py\", line 23, in <module>\n",
      "    dag=dag\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 179, in apply_defaults\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/operators/bash.py\", line 143, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 179, in apply_defaults\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 617, in __init__\n",
      "    self.dag = dag or DagContext.get_current_dag()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 723, in __setattr__\n",
      "    super().__setattr__(key, value)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 774, in dag\n",
      "    dag.add_task(self)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/dag.py\", line 1603, in add_task\n",
      "    raise AirflowException(\"Task is missing the start_date parameter\")\n",
      "airflow.exceptions.AirflowException: Task is missing the start_date parameter\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1e65623dc7 -> bdaa763e6c56, Make xcom value column a large binary\n",
      "INFO  [alembic.runtime.migration] Running upgrade bdaa763e6c56 -> 947454bf1dff, add ti job_id index\n",
      "INFO  [alembic.runtime.migration] Running upgrade 947454bf1dff -> d2ae31099d61, Increase text size for MySQL (not relevant for other DBs' text types)\n",
      "INFO  [alembic.runtime.migration] Running upgrade d2ae31099d61 -> 0e2a74e0fc9f, Add time zone awareness\n",
      "INFO  [alembic.runtime.migration] Running upgrade d2ae31099d61 -> 33ae817a1ff4, kubernetes_resource_checkpointing\n",
      "INFO  [alembic.runtime.migration] Running upgrade 33ae817a1ff4 -> 27c6a30d7c24, kubernetes_resource_checkpointing\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27c6a30d7c24 -> 86770d1215c0, add kubernetes scheduler uniqueness\n",
      "INFO  [alembic.runtime.migration] Running upgrade 86770d1215c0, 0e2a74e0fc9f -> 05f30312d566, merge heads\n",
      "INFO  [alembic.runtime.migration] Running upgrade 05f30312d566 -> f23433877c24, fix mysql not null constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade f23433877c24 -> 856955da8476, fix sqlite foreign key\n",
      "INFO  [alembic.runtime.migration] Running upgrade 856955da8476 -> 9635ae0956e7, index-faskfail\n",
      "INFO  [alembic.runtime.migration] Running upgrade 9635ae0956e7 -> dd25f486b8ea, add idx_log_dag\n",
      "INFO  [alembic.runtime.migration] Running upgrade dd25f486b8ea -> bf00311e1990, add index to taskinstance\n",
      "INFO  [alembic.runtime.migration] Running upgrade 9635ae0956e7 -> 0a2a5b66e19d, add task_reschedule table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a2a5b66e19d, bf00311e1990 -> 03bc53e68815, merge_heads_2\n",
      "INFO  [alembic.runtime.migration] Running upgrade 03bc53e68815 -> 41f5f12752f8, add superuser field\n",
      "INFO  [alembic.runtime.migration] Running upgrade 41f5f12752f8 -> c8ffec048a3b, add fields to dag\n",
      "INFO  [alembic.runtime.migration] Running upgrade c8ffec048a3b -> dd4ecb8fbee3, Add schedule interval to dag\n",
      "INFO  [alembic.runtime.migration] Running upgrade dd4ecb8fbee3 -> 939bb1e647c8, task reschedule fk on cascade delete\n",
      "INFO  [alembic.runtime.migration] Running upgrade 939bb1e647c8 -> 6e96a59344a4, Make TaskInstance.pool not nullable\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6e96a59344a4 -> d38e04c12aa2, add serialized_dag table\n",
      "INFO  [alembic.runtime.migration] Running upgrade d38e04c12aa2 -> b3b105409875, add root_dag_id to DAG\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6e96a59344a4 -> 74effc47d867, change datetime to datetime2(6) on MSSQL tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 939bb1e647c8 -> 004c1210f153, increase queue name size limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade c8ffec048a3b -> a56c9515abdc, Remove dag_stat table\n",
      "INFO  [alembic.runtime.migration] Running upgrade a56c9515abdc, 004c1210f153, 74effc47d867, b3b105409875 -> 08364691d074, Merge the four heads back together\n",
      "INFO  [alembic.runtime.migration] Running upgrade 08364691d074 -> fe461863935f, increase_length_for_connection_password\n",
      "INFO  [alembic.runtime.migration] Running upgrade fe461863935f -> 7939bcff74ba, Add DagTags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7939bcff74ba -> a4c2fd67d16b, add pool_slots field to task_instance\n",
      "INFO  [alembic.runtime.migration] Running upgrade a4c2fd67d16b -> 852ae6c715af, Add RenderedTaskInstanceFields table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 852ae6c715af -> 952da73b5eff, add dag_code table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 952da73b5eff -> a66efa278eea, Add Precision to execution_date in RenderedTaskInstanceFields table\n",
      "INFO  [alembic.runtime.migration] Running upgrade a66efa278eea -> da3f683c3a5a, Add dag_hash Column to serialized_dag table\n",
      "INFO  [alembic.runtime.migration] Running upgrade da3f683c3a5a -> 92c57b58940d, Create FAB Tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 92c57b58940d -> 03afc6b6f902, Increase length of FAB ab_view_menu.name column\n",
      "INFO  [alembic.runtime.migration] Running upgrade 03afc6b6f902 -> cf5dc11e79ad, drop_user_and_chart\n",
      "INFO  [alembic.runtime.migration] Running upgrade cf5dc11e79ad -> bbf4a7ad0465, Remove id column from xcom\n",
      "INFO  [alembic.runtime.migration] Running upgrade bbf4a7ad0465 -> b25a55525161, Increase length of pool name\n",
      "INFO  [alembic.runtime.migration] Running upgrade b25a55525161 -> 3c20cacc0044, Add DagRun run_type\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3c20cacc0044 -> 8f966b9c467a, Set conn_type as non-nullable\n",
      "INFO  [alembic.runtime.migration] Running upgrade 8f966b9c467a -> 8d48763f6d53, add unique constraint to conn_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade 8d48763f6d53 -> e38be357a868, Add sensor_instance table\n",
      "INFO  [alembic.runtime.migration] Running upgrade e38be357a868 -> b247b1e3d1ed, Add queued by Job ID to TI\n",
      "INFO  [alembic.runtime.migration] Running upgrade b247b1e3d1ed -> e1a11ece99cc, Add external executor ID to TI\n",
      "INFO  [alembic.runtime.migration] Running upgrade e1a11ece99cc -> bef4f3d11e8b, Drop KubeResourceVersion and KubeWorkerId\n",
      "INFO  [alembic.runtime.migration] Running upgrade bef4f3d11e8b -> 98271e7606e2, Add scheduling_decision to DagRun and DAG\n",
      "INFO  [alembic.runtime.migration] Running upgrade 98271e7606e2 -> 52d53670a240, fix_mssql_exec_date_rendered_task_instance_fields_for_MSSQL\n",
      "INFO  [alembic.runtime.migration] Running upgrade 52d53670a240 -> 364159666cbd, Add creating_job_id to DagRun table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 364159666cbd -> 45ba3f1493b9, add-k8s-yaml-to-rendered-templates\n",
      "INFO  [alembic.runtime.migration] Running upgrade 45ba3f1493b9 -> 849da589634d, Prefix DAG permissions.\n",
      "INFO  [alembic.runtime.migration] Running upgrade 849da589634d -> 2c6edca13270, Resource based permissions.\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2c6edca13270 -> 61ec73d9401f, Add description field to connection\n",
      "INFO  [alembic.runtime.migration] Running upgrade 61ec73d9401f -> 64a7d6477aae, fix description field in connection to be text\n",
      "INFO  [alembic.runtime.migration] Running upgrade 64a7d6477aae -> e959f08ac86c, Change field in DagCode to MEDIUMTEXT for MySql\n",
      "INFO  [alembic.runtime.migration] Running upgrade e959f08ac86c -> 82b7c48c147f, Remove can_read permission on config resource for User and Viewer role\n",
      "INFO  [alembic.runtime.migration] Running upgrade 82b7c48c147f -> 449b4072c2da, Increase size of connection.extra field to handle multiple RSA keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 449b4072c2da -> 8646922c8a04, Change default pool_slots to 1\n",
      "INFO  [alembic.runtime.migration] Running upgrade 8646922c8a04 -> 2e42bb497a22, rename last_scheduler_run column\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2e42bb497a22 -> 90d1635d7b86, Increase pool name size in TaskInstance\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90d1635d7b86 -> e165e7455d70, add description field to variable\n",
      "INFO  [alembic.runtime.migration] Running upgrade e165e7455d70 -> a13f7613ad25, Resource based permissions for default FAB views.\n",
      "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
      "INFO  [airflow.models.dagbag.DagBag] Filling up the DagBag from /opt/ml/airflow/dags\n",
      "ERROR [airflow.models.dagbag.DagBag] Failed to import: /opt/ml/airflow/dags/simple_dag.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/dagbag.py\", line 317, in _load_modules_from_file\n",
      "    loader.exec_module(new_module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/opt/ml/airflow/dags/simple_dag.py\", line 23, in <module>\n",
      "    dag=dag\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 179, in apply_defaults\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/operators/bash.py\", line 143, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 179, in apply_defaults\n",
      "    result = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 617, in __init__\n",
      "    self.dag = dag or DagContext.get_current_dag()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 723, in __setattr__\n",
      "    super().__setattr__(key, value)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/baseoperator.py\", line 774, in dag\n",
      "    dag.add_task(self)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/airflow/models/dag.py\", line 1603, in add_task\n",
      "    raise AirflowException(\"Task is missing the start_date parameter\")\n",
      "airflow.exceptions.AirflowException: Task is missing the start_date parameter\n"
     ]
    }
   ],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 간단한 Python Operator 기반으로 한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/python_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "import time\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_python_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple python pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "#Python 함수에 변수를 받을 때는 *args = [], *kwargs = {} 형태로 받을 수 있다.\n",
    "def sleep(**kwargs):\n",
    "    delta = kwargs['delta']\n",
    "    time.sleep(delta)\n",
    "    print(\"Slept for {} seconds\".format(delta))\n",
    "    \n",
    "\n",
    "task_1 = PythonOperator(\n",
    "    task_id='task_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = PythonOperator(\n",
    "    task_id='task_2',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 30}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = PythonOperator(\n",
    "    task_id='task_3',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 5}, #kwargs 형태로 전달\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/python_combined_dag.py\n",
    "# 이 실습에서는 database backend를 sqlite를 사용하기 때문에 병렬 처리가 되지 않지만, \n",
    "# mysql과 같은 데이터베이스를 사용하면 병렬 처리가 가능합니다.\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "import time\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'combined_python_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A combined python pipeline',\n",
    "    schedule_interval=None,\n",
    "    concurrency = 2\n",
    ")\n",
    "\n",
    "def sleep(**kwargs):\n",
    "    delta = kwargs['delta']\n",
    "    time.sleep(delta)\n",
    "    print(\"Slept for {} seconds\".format(delta))\n",
    "    \n",
    "\n",
    "task_1 = PythonOperator(\n",
    "    task_id='task_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10},\n",
    "    dag=dag\n",
    ")\n",
    "task_2_1 = PythonOperator(\n",
    "    task_id='task_2_1',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 30},\n",
    "    dag=dag\n",
    ")\n",
    "task_2_2 = PythonOperator(\n",
    "    task_id='task_2_2',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 10},\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = PythonOperator(\n",
    "    task_id='task_3',\n",
    "    python_callable=sleep,\n",
    "    op_kwargs={'delta': 5},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# task_2_1과 task_2_2가 만족이 되어야 task_3dl tlfgod\n",
    "# 현재는 SequentialExecutor 세팅이지만, mysql, postresql 등을 연동하여 LocalExecutor 사용시 병렬 처리 가능!\n",
    "task_1 >> [task_2_1, task_2_2] >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스케쥴을 사용한 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/scheduled_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "# interval 세팅은 다음 링크 참고\n",
    "# https://airflow.apache.org/docs/apache-airflow/1.10.1/scheduler.html#dag-runs\n",
    "# 아래 예제는 매 5분마다\n",
    "dag = DAG(\n",
    "    'simple_scheduled_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple scheduled pipeline',\n",
    "    schedule_interval='*/5 * * * *',\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한번에 한 run만 실행이 되고, 지난 시간 부분을 채우지 않는 Dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/strict_scheduled_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "# 1분 마다 하되, scheduler에 등록된 시점을 기준으로만, 그리고 한번에 최대 1개의 run만\n",
    "dag = DAG(\n",
    "    'strict_scheduled_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A strict scheduled pipeline',\n",
    "    schedule_interval='*/1 * * * *',\n",
    "    is_paused_upon_creation=False, #등록되면 바로 활성화\n",
    "    catchup = False, # 시작시점(start_date) 부터 채워 넣지 않기\n",
    "    max_active_runs=1 #한번에 한 run만\n",
    ")\n",
    "\n",
    "\n",
    "task_1 = BashOperator(\n",
    "    task_id='task_1',\n",
    "    bash_command=\"echo 1\",\n",
    "    dag=dag\n",
    ")\n",
    "task_2 = BashOperator(\n",
    "    task_id='task_3',\n",
    "    bash_command=\"echo 2\",\n",
    "    dag=dag\n",
    ")\n",
    "task_3 = BashOperator(\n",
    "    task_id='task_2',\n",
    "    bash_command=\"echo 3\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task_1 >> task_2 >> task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 실제 실습 Dag\n",
    "- 강의용 코드기 때문에 한 파일에 관련된 함수들을 모두 다 넣었습니다.\n",
    "- 아래 코드는 실행용이 아닌 airflow dags 폴더에 저장이 되도록해놓은 코드입니다.\n",
    "- 데이터 수집 - 훈련 - 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/airflow/dags/airflow_example.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import mlflow\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import random\n",
    "import requests\n",
    "\n",
    "#####START ML CODE#####\n",
    "def collect_data():\n",
    "    #40%의 확률로 에러가 발생하도록 설정해놨습니다.\n",
    "    if random.randint(0,10) < 4:\n",
    "        raise Exception(\"Fake ERROR: Failed to Download!\")\n",
    "    url = 'https://s3.amazonaws.com/img-datasets/mnist.npz'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('mnist.npz', 'wb').write(r.content)\n",
    "    \n",
    "# context는 operator간의 값들을 공유하기 위함\n",
    "def train(**context):\n",
    "    config = AttrDict(context['dag_run'].conf)\n",
    "\n",
    "    mlflowInit(config)\n",
    "    f = np.load('mnist.npz')\n",
    "\n",
    "    sample = 5000\n",
    "    X_train, y_train = f['x_train'][:sample], f['y_train'][:sample]\n",
    "    X_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    X_train, X_test = X_train.reshape(X_train.shape[0],-1), X_test.reshape(X_test.shape[0],-1)\n",
    "    \n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    result = (prediction == y_test).mean()\n",
    "    print(result)\n",
    "    mlflow.log_metric('acc',result)\n",
    "    \n",
    "    mlflow.sklearn.log_model(clf, 'save_model')\n",
    "    model_path = mlflow.get_artifact_uri().replace('file://', '')\n",
    "    \n",
    "    #중요: task간의 값들을 전달하는 방법\n",
    "    #xcom == cross communication\n",
    "    context['ti'].xcom_push(key='model_path', value=model_path)\n",
    "    \n",
    "    mlflow.end_run()\n",
    "\n",
    "def mlflowInit(config):\n",
    "    try:\n",
    "        mlflow.create_experiment(name=config.experiment)\n",
    "    except:\n",
    "        print('Exist experiment')\n",
    "\n",
    "    mlflow.set_experiment(config.experiment)\n",
    "\n",
    "    mlflow.start_run()\n",
    "\n",
    "    mlflow.set_tag('version', config.version)\n",
    "    mlflow.log_params(config)\n",
    "#####END ML CODE#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####START DAG CODE#####\n",
    "default_args = {\n",
    "    'owner': 'Boost Kim',\n",
    "    'depends_on_past': True,\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 4,\n",
    "    'retry_delay': timedelta(seconds=20)\n",
    "}\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'ml_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple Machine Learning pipeline',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "\n",
    "download_images = PythonOperator(\n",
    "    task_id='collect_data',\n",
    "    python_callable=collect_data,\n",
    "    retries=3,\n",
    "    dag=dag,\n",
    ")\n",
    "train = PythonOperator(\n",
    "    task_id='train',\n",
    "    depends_on_past=True,\n",
    "    python_callable=train,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "#airflow ti 변수 활용 (jinja tempalte 방식)\n",
    "#https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html#macros-reference\n",
    "serve = BashOperator(\n",
    "    task_id='serve',\n",
    "    depends_on_past=False,\n",
    "    bash_command=\"mlflow models serve -m {{ ti.xcom_pull(key='model_path') }}/save_model --no-conda -h 0.0.0.0 -p 8889 &\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_images >> train >> serve\n",
    "\n",
    "#####END DAG CODE#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!airflow db reset -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airflow GUI 상에서 해당 DAG를 trigger를 하면서 입력해주는 configuration\n",
    "#{\"version\": 0.1,\"experiment\": \"mlflow-airflow\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서빙되는 모델 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='data/mnist_5.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import requests, json\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('data/mnist_5.jpg')\n",
    "image_data = np.array(img, dtype='uint8').reshape(-1).tolist()\n",
    "\n",
    "url = 'http://localhost:8889/invocations'\n",
    "\n",
    "data = {\n",
    "    \"columns\": [i for i in range(0, len(image_data))],\n",
    "    \"data\": [image_data]\n",
    "}\n",
    "headers = {\n",
    "    'content-type':'application/json'\n",
    "}\n",
    "res = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print('Predicted From Server:',json.loads(res.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#백그라운드로 돌고있는 웹서버를 종료하기 위한 코드\n",
    "!apt-get install lsof\n",
    "!kill -9 `lsof -t -i:8889`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}